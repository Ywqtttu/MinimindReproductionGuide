# LLM理论

## MLP/感知器模型

神经网络就是一系列数字计算的堆砌。

叠加：一个模型的输出是另外一个模型的输入

简单模型的叠加无法生成更强大的模型，比如线性模型

叠加非线性变换的模型，就有可能1+1>2

由于这个堆砌很像人的神经网络，故而得名。

### 多层感知器MLP

同一层的神经元之间不连接

相邻层之间的神经元会连接

随机梯度下降法

前面复杂的层是在特征提取，可以在最后加一个逻辑回归的层，这样能够组装起来解决其他复杂问题。

### 优化神经网络

#### 监控模型

**激活函数过热和神经细胞坏死**

对于不同的数据点，有些数据点可能会过热，导致梯度失效，另外一些反之。

而当所有数据点都过热时，情况就很糟糕。

**过热的数学原理**

在计算梯度时，也会有激活函数导数的参与

当激活函数的导数的值过小时，会导致梯度被计算机归0

**梯度消失**

越远离输出层的层，越难训练，因为根据反向传播的原理，它最终的偏导数会受到其他层的偏导数的缩小。而过于靠近输出层的层就有相反的问题。

通过激活函数的状态来监控模型

==**另一个指标：模型差比值**==

![image-20250410163821681](C:\Users\19736\AppData\Roaming\Typora\typora-user-images\image-20250410163821681.png)

手动编写if监视激活函数过热

#### 优化模型

**tanh函数**

使用tanh函数作激活函数，因为在0附近，它的偏导数几乎为1，能够避免激活函数过热的问题

**模型初始化**

 初始化时的参数过大，会导致误差变大，那为什么不初始化为全0？

这会导致所有神经元的结果都相同，会影响迭代。

*优化方法*

缩小参数的标准差，将每一个层的参数除以一个1/len(features)**0.5

奇妙的值，给tanh的学习速率，5/3

**但是其实更推荐，在输入之前就对数据进行归一化，因为参数对模型的影响极大，稍有不慎就会翻车**

**归一化层**|归一化层

但是归一化之后由于层输出变化被缩小了，也可能影响模型效果——引入gamma/beta

self.out = self.gamma * xhat + self.beta

这样每一层就不完全独立了

### 接下来战术总结一下Torch的建模流程

首先需要明确的是，torch本身提供了很多简便的数据处理的函数，与numpy类似，但是需要相当的时间去熟悉

通常可以继承torch.nn.Module类来设计神经网络，也可以直接使用torch.nn.functional，前者能够在网络中留下相应的layer以便其他操作，而后者只会接收输入，给出输出

和TensorFlow类似的，在网络比较简单的情况下，也可以直接使用nn.Sequential方法构造简单的神经网络

Module类中的forward方法是必须被显式实现的，它定义了网络层的传递逻辑和传播方法，但是在实例化之后，不能直接使用.forward，因为在Module类中，forward是被__call__调用的，它也依赖于__call__中定义的一些东西，不能单独使用。

Module默认的是前向传播，要是网络需要其他传播方式，需要自定义，这一步挺复杂的

#### torch.nn.Sequential

默认的使用方法就是,model = nn.Sequential(network1,

network2,

network3...)

通常需要重命名，则有

model  = nn.Sequential(OrderedDict([

('name1', network1),

('name2', network2),

...

])

)

或者，也可以在建立一个model之后，手动添加。

model = nn.Sequential()

model.add_module("name1", network1)

model.add_module("name2", network2)

...

卷积神经网络（CNN）等的module结构又有所不同了，这里只能简单讲讲通用的范式。

#### 简单来说，最重要的就是两个方法，__init__初始化网络的层结构，__forward__定义网络的传播逻辑。

### 调用transform的测试数据

#### 训练过程

关键词记录，在训练之前会切换模型为model.train()

而预测结果时，要切换为model.eval()

因为torch的动态图计算以及其他的特性，网络的参数或者其他属性未必需要显式写出就可以在各种实例间流转，这是它方便的地方，也是它迷惑人的地方。

#### optimizer

PyTorch 如何使用优化器

```python3
Example:
>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
>>> optimizer.zero_grad()
>>> loss_fn(model(input), target).backward()
>>> optimizer.step()
```

往optim中传入参数parameters，接着重置optimizer的梯度，最后使用反向传播计算各参数的梯度，迭代调参。

#### criterion

loss会自动关联网络中层的参数，因此不需要额外输入parameters，也可以通过反向传播计算迭代

criterion.backward

再回头思考一下，现在已经明确了如何搭建神经网络了，而尚不明确的就是训练模型的过程

其实也蛮简单，无非就是，先准备好数据，也就是张量化，传递给模型输入，然后得到输出，这就算学习了一次了。

接着记录本次学习的一些指标，比如准确率和误差率。

然后再拎到测试集里去跑一遍，记录准确率和误差率（或者需要的指标）

问题在于，参数学习这个步骤，放到哪去了？

### 卷积神经网络

先前做图像识别时，图片数据是已经被数据化的了。

现在情况不同了，需要自己数字化图片。

把图片压缩为二维张量。

但是对于彩色图片，就要是三维的张量了。

**训练集**

**验证集**

**测试集**

在比较复杂的模型中，通常使用的也是__init__和__forward__方法来构造模型

对于多层感知器，直接用nn.Sequetntial函数来实现即可

在开始训练模型之前就实现评估模型

**生成模型-评估模型-训练模型**	

评估时使用若干批次效果的平均值来评估，此时不需要记录梯度的依赖关系。

实现评估模型的代码后就直接在训练模型的循环中加入评估模型的环节，以监控模型的训练

#### **对抗过拟合**

**随机失活、添加惩罚项**

前者则就是随机剔除某神经元的结果。

nn.Dropout提供的随机失活，在评估阶段不会操作。

同时批归一化组件也相同，因此建议显式指出模型的状态。

**惩罚项**

在损失函数中，加上跟模型参数相关的损失项，使得限制模型参数过于夸张这一目标也被加入优化器

### ！来了，CNN

对于MLP，需要将三维数据打平才能参加模型计算，但显然的，这个操作让我们丢失了数据的特征。

而在CNN中，则不必如此。

输入层分为多个神经元，一个神经元可以代表数据的一个维度。

在MLP中，不同层的对应关系没有那么单纯。

#### **卷积计算+池化运算**

##### 卷积层的设计

以图像识别为例。

本地感受野，从左到右，从上到下，依次移动这个窗口，再输出到隐藏层中。

**Conv2d**

一个卷积层其实是一个提取特征的操作

in:3 out:4 输出的4个通道的的结果是in的3个通道的结果的加和

步幅stride设得太大的话会丢失数据

接着会有一个padding来防止这种情况发生

**池化层**

模拟视觉的模糊处理

完整结构：

conv-relu-pool

其他的数据，只要能被转化为三维张量，也可以使用卷积神经网络，比如文本分类

在一定范围内，增加模型深度确实可以提升模型效果

#### 残差连接

跳过神经层而将梯度直接传播到前面的层。

输出表述为输入和输入的一个非线性变换的线性叠加，没用新的公式，没有新的理论，只是换了一种新的表达。

为什么残差连接能够有效缓解深层网络的反向传播问题？因为每一层的梯度都多了一个恒等的1，因此也能够进行有效的传播

#### 对高维数据进行归一化处理

以每一个通道为区分维度，计算该通道下所有点的均值和方差

### CNN战术总结

最初是为了解决图像识别问题提出的。

- 设计了一个卷积核（本地识别野），在图像矩阵上滑动，提取特征。

- 设计了一个池化层，精简特征，可以通过选择窗口内的最大值或者平均值等操作来实现。

### RNN

RNN 的核心特征是它包含**循环连接**（或称反馈连接）。这意味着网络中的信息不仅向前传播到下一层，还会向后传播到同一层的先前时间步，或者说，当前时间步的计算会**依赖于前一个时间步的隐藏状态**。这种循环结构使得 RNN 能够处理长度不定的序列，并在处理序列的每个元素时考虑之前元素的信息。

RNN 的计算围绕着它的**隐藏状态（hidden state）**进行。在每个时间步 t，RNN 会接收当前时间步的输入 xt 和前一个时间步的隐藏状态 ht−1，然后计算出当前时间步的隐藏状态 ht 和输出 ot

最基本的 RNN 单元的计算公式如下：

- **隐藏状态更新：**

  $$ h_{t}=tanh(W_{hh}h_{t−1}+W_{xh}x_{t}+b_{h})$$

   这里：

  - ht 是当前时间步的隐藏状态。
  - ht−1 是前一个时间步的隐藏状态（在第一个时间步通常初始化为零向量）。
  - xt 是当前时间步的输入。
  - Whh 是连接隐藏状态到隐藏状态的权重矩阵。
  - Wxh 是连接输入到隐藏状态的权重矩阵。
  - bh 是隐藏状态的偏置项。
  - tanh 是激活函数（也可以是 ReLU 或其他激活函数）。

## 自然语言处理步骤

- 语言数字化

- 模型选择

- 设计学习框架

### 分词器，是一个字典，排序好的字典

有很多词元(token)，和输入中的“文字”一一对应，会预留一个特殊的token来匹配未知的字符。

### 如何构建分词器

对于英文来说。

- 以字母分词，单字母无意义

- 以单词分词，单词太多难以训练，约26w，但token数量大于5w时，模型难以训练。

因此选择折中的做法：字节对编码。

### 字节对编码

- 初始化，拆解为一个个字母
- 合并，找到出现频率最高的组合，加入字典
- 循环合并

但只能处理字母语言

### 字节级-字节对编码

初始化的时候，以字节为单位存储，初始字典就是一个8位bit（256大小），就是直接用字符的二进制码做元素。

huggingface在github上提供了一个脚本，讲解了如何训练新的分词器。

### 学习框架

采取迁移学习而非传统的学习，

**在预训练阶段主要学习语言**

训练模式：seq2seq/autoencoding model/autoregressive model

### 模型选择和语言数字化

embedding(分词器与文本嵌入)

 希望用更少的特征，来表示文字

希望从(T, VS) ---> (T, C)

最简单的一个方法——线性变换

**Colab**

文字(T)--分词器---> 张量元素(T)---embedding---> （T,C）

(B, T) --------分词器-----> (B, T) ------embedding-------> （B, T, C）

B: 文本个数

T: 文本长度

C: 特征个数

C 是自己设计的。

### 自回归模式

为了防止文本背景过长，可以固定窗口长度。

#### 迁移学习

embedding的参数

nn.Embedding(batchsize, featuresize)

input的batchsize不能超过128

因为MLP无法捕捉窗口外和窗口间的关联关系，且模型结构死板，运算效率低下

所以出现了RNN

4种学习框架

![image-20250415150947234](C:\Users\19736\AppData\Roaming\Typora\typora-user-images\image-20250415150947234.png)

![image-20250415151126402](C:\Users\19736\AppData\Roaming\Typora\typora-user-images\image-20250415151126402.png)

相比于MLP，会将数据一个个反复输入层，所有数据共享参数，但是每一次输入后的输出会和下一份数据在一起作为输入再输入层

 初始的RNN也有同样的梯度消散的问题

——短期记忆

### 深度循环神经网络

将输入从(T, C)变成(B, T, C)，在B的维度上并行计算。

## 注意力机制

将编码器中计算后得到的每个字符的隐藏状态，进行加和平均计算，得到一个张量

也可以用在自回归和自编码模式

![image-20250424192429295](C:\Users\19736\AppData\Roaming\Typora\typora-user-images\image-20250424192429295.png)

简明理解：

查询为Query，每次接受查询之后，会到Key中去逐个计算相似度，返回相似度高的Key的Value，大致是这个意思。

多头注意力：会准备不同侧重点的Keys，在每次Query的时候分头去寻找重点，然后汇总返回Values

X[tokens_size, d_model]

Wq Wk Wv 初始化都是相对随机的

形状都是[d_model, d_model/ heads_size]

然后会线性变化得到

倾向只用解码器进行自回归学习

结构简单、训练容易、应用场景丰富

## LSTM 长短期记忆神经网络

**LSTM（长短时记忆网络）是一种常用于处理序列数据的深度学习模型，与传统的 RNN（循环神经网络）相比，LSTM引入了三个门（ 输入门、遗忘门、输出门）和一个 细胞状态（cell state），这些机制使得LSTM能够更好地处理序列中的长期依赖关系。**

### 遗忘门

x投入层，和ht一起计算，经过sigmoid，得到一个(0, 1)的向量，为0就代表该数据点要被抛弃，为1就代表该数据点需要被保留，参加下一次计算。

### 输入门

被遗忘门筛选后，剩下的内容被加入下一次计算。

### 输出门

输出计算结果

