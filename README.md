# MiniMind复现指南

基于jingyaogong大佬的MiniMind仓库（https://github.com/jingyaogong/minimind），探讨如何从0开始复现并训练一个轻量化LLM（其实就是MiniMind-Dense）。

本项目旨在通过复现轻量化大语言模型 MiniMind，提供一个清晰、可操作的实践指南，帮助开发者理解和掌握从零开始训练一个轻量级 LLM 的关键流程与技术细节。逐步实现了模型结构设计、数据预处理、训练 pipeline 构建以及基础推理演示，力求在代码和文档中体现透明性和可复现性。

过程中，我们注重对原项目设计理念的还原，同时适当加入了训练优化、代码注释和工具脚本，以提升实验可重复性和易用性。本项目特别适合对大模型训练感兴趣但计算资源有限的开发者、学习者，也可作为 LLM 教学和实践的参考项目。

我们欢迎社区开发者参与测试、反馈问题或提交改进，共同完善这个开源实现。

**尤其感谢 jingyaogong 及其团队的巨大贡献！**

## **注意**

由于笔者本地算力所限，所有源码均只在Colab上运行过，下载到本地未必能即点即用，需要修改对应路径。

由于训练完成的模型参数.pth文件和训练数据集.jsonl文件过大，不便于GitHub上存取、更新，另附网盘链接如下，可能会持续更新。

## 文件夹说明

### BreakdownProcess

主文件夹，包含从模型搭建到预训练环节的全部详细教程，和后续的SFT, LoRA，RLHF（DPO）环节复现代码。

- dateset

  存储所有Dataset子类（如PretrainDataset等）和训练数据集.jsonl文件。

- eval

  存储模型评估脚本。

- model

  存储MiniMind模型本体、MiniMindLoRA模型本体和分词器模型。

  - tutorials

    存储建模步骤分解教程jnb

- trainer

  存储各种训练器，比如预训练器，微调训练器，LoRA训练器等。

  - tutorial

    存储训练步骤分解教程jnb

### 经典论文简单复现

包含了对《Attention is all you need》一文中提出的经典Transformer（带编码器）架构的复现，和BERT，GPT1对应结构模型的复现。

# 更新日志

## 0901全面更新

全面更新了项目结构，删去所有不必要的文件（如之前的Notes等文件夹），仅留下以下两个核心部分（以下为概述，详情见上文件说明）。

- BreakdownProcess
  - MiniMind的全流程拆解（包括到预训练环节的详细教程Jnb文件和其他环节的复现代码）
- 经典论文简单复现
  - 复现了《Attention is all you need》中的经典Transformer架构模型和BERT及GPT1的论文对应结构的模型。

## 0810紧急补丁

* Codes_pure这个文件夹本来是留着存可命令行交互的纯代码的，但是笔者命令行相关库功夫没学到家（），一时不能完成，因此暂时置空。
* 优化了可读性、更换了训练集。

## 0810之前

- 更新模型基本结构和步骤拆解教程。
