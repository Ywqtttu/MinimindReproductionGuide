{"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import random"],"metadata":{"id":"Q12MtZIjeCey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ----------------- 第1部分：模型架构 -----------------\n","# 这是一个简化的Transformer编码器块\n","class TransformerEncoderBlock(nn.Module):\n","    def __init__(self, d_model, n_heads):\n","        super().__init__()\n","        # BERT使用非掩码的自注意力，实现双向\n","        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=0.1)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(0.1)\n","\n","        # 前馈网络\n","        self.ffn = nn.Sequential(\n","            nn.Linear(d_model, 4 * d_model),\n","            nn.GELU(),\n","            nn.Linear(4 * d_model, d_model),\n","            nn.Dropout(0.1)\n","        )\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # 无需掩码，因为BERT是双向的\n","        attn_output, _ = self.self_attn(x, x, x)\n","        x = self.norm1(x + self.dropout1(attn_output))\n","\n","        ffn_output = self.ffn(x)\n","        x = self.norm2(x + self.dropout2(ffn_output))\n","        return x"],"metadata":{"id":"3Ngjl9HPd7UD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 整个BERT模型\n","class BERT(nn.Module):\n","    def __init__(self, vocab_size, d_model, n_layers, n_heads, max_seq_len):\n","        super().__init__()\n","\n","        # 1. 词嵌入 (Token Embeddings)\n","        self.token_embedding = nn.Embedding(vocab_size, d_model)\n","        # 2. 位置嵌入 (Position Embeddings)\n","        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n","        # 3. 句子/片段嵌入 (Segment Embeddings)\n","        self.segment_embedding = nn.Embedding(2, d_model)\n","\n","        self.transformer_blocks = nn.ModuleList([\n","            TransformerEncoderBlock(d_model, n_heads) for _ in range(n_layers)\n","        ])\n","\n","        # MLM任务头\n","        self.mlm_head = nn.Linear(d_model, vocab_size)\n","\n","        # NSP任务头\n","        self.nsp_head = nn.Linear(d_model, 2)\n","\n","    def forward(self, input_ids, segment_ids):\n","        seq_len = input_ids.shape[1]\n","\n","        # 论文输入表示：token + segment + position\n","        token_emb = self.token_embedding(input_ids)\n","        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)\n","        position_emb = self.position_embedding(position_ids)\n","        segment_emb = self.segment_embedding(segment_ids)\n","\n","        input_emb = token_emb + position_emb + segment_emb\n","\n","        x = input_emb\n","        for block in self.transformer_blocks:\n","            x = block(x)\n","\n","        return x\n"],"metadata":{"id":"UG5k_Xtkd-_g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ----------------- 第2部分：预训练数据准备 -----------------\n","def create_pretraining_data(tokens, vocab, max_seq_len):\n","    # 这部分是伪代码，因为实际数据处理非常复杂\n","    # 它展示了MLM和NSP任务的数据准备逻辑\n","\n","    # NSP任务：50%的概率是下一句，50%的概率是随机句\n","    is_next = random.random() < 0.5\n","    if is_next:\n","        # 随机选择一个连续的句子作为下一句\n","        # ...\n","        pass\n","    else:\n","        # 从语料库中随机选择一个不相关的句子\n","        # ...\n","        pass\n","\n","    # 将句子A和句子B拼接，并加入特殊token\n","    # input_ids = [CLS] + sent_A_tokens + [SEP] + sent_B_tokens + [SEP]\n","    # segment_ids = [0] + ... + [0] + [1] + ... + [1]\n","\n","    # MLM任务：掩码15%的tokens\n","    masked_labels = []\n","    masked_input = list(input_ids)\n","\n","    # 论文中的掩码策略：80/10/10\n","    num_to_mask = max(1, int(len(input_ids) * 0.15))\n","    mask_indices = random.sample(range(1, len(input_ids) - 1), num_to_mask) # 排除CLS和SEP\n","\n","    for i in mask_indices:\n","        masked_labels.append(input_ids[i])\n","        prob = random.random()\n","        if prob < 0.8:\n","            masked_input[i] = vocab['[MASK]']\n","        elif prob < 0.9:\n","            # 保持不变\n","            pass\n","        else:\n","            masked_input[i] = random.choice(list(vocab.keys()))\n","\n","    return masked_input, segment_ids, masked_labels, is_next"],"outputs":[],"execution_count":null,"metadata":{"id":"kAUrq2Nld1EJ"}},{"cell_type":"code","source":["# ----------------- 第3部分：预训练循环 -----------------\n","\n","def pretrain_bert(model, dataloader, optimizer, num_epochs, vocab):\n","    model.train()\n","    mlm_criterion = nn.CrossEntropyLoss()\n","    nsp_criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(num_epochs):\n","        for batch in dataloader:\n","            # batch 包含了预处理好的数据\n","            input_ids, segment_ids, mlm_labels, nsp_labels = batch\n","\n","            # 正向传播\n","            bert_output = model(input_ids, segment_ids)\n","\n","            # --- 任务1: 掩码语言建模 (MLM) ---\n","            # 从BERT输出中提取被掩码位置的表示\n","            mlm_positions = (mlm_labels != -1).nonzero(as_tuple=True)[0] # 找到被掩码的token位置\n","            mlm_outputs = bert_output[mlm_positions]\n","\n","            # 使用MLM头预测被掩码的词\n","            mlm_logits = model.mlm_head(mlm_outputs)\n","\n","            # 计算MLM损失\n","            mlm_labels = mlm_labels[mlm_positions]\n","            mlm_loss = mlm_criterion(mlm_logits, mlm_labels)\n","\n","            # --- 任务2: 下一句预测 (NSP) ---\n","            # 提取[CLS] token的表示，它包含了整个序列的上下文\n","            cls_output = bert_output[:, 0, :]\n","\n","            # 使用NSP头进行二分类预测\n","            nsp_logits = model.nsp_head(cls_output)\n","\n","            # 计算NSP损失\n","            nsp_loss = nsp_criterion(nsp_logits, nsp_labels)\n","\n","            # 论文中没有显式提到加权，通常是简单相加\n","            total_loss = mlm_loss + nsp_loss\n","\n","            # 反向传播和优化\n","            optimizer.zero_grad()\n","            total_loss.backward()\n","            optimizer.step()\n","\n","            print(f\"Epoch {epoch}, Total Loss: {total_loss.item()}\")"],"metadata":{"id":"IPYuws7VeMzy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}