{"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# ----------------- 第1部分：模型架构 -----------------\n# 这是一个简化的Transformer解码器块，不包含所有细节\n# 核心是带掩码的自注意力机制\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        # 自注意力层：带掩码是关键\n        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=0.1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(0.1)\n        \n        # 前馈网络\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.GELU(),\n            nn.Linear(4 * d_model, d_model),\n            nn.Dropout(0.1)\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask):\n        # 掩码自注意力\n        attn_output, _ = self.self_attn(x, x, x, attn_mask=mask)\n        x = self.norm1(x + self.dropout1(attn_output))\n        \n        # 前馈网络\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout2(ffn_output))\n        return x\n\n# 整个GPT模型\nclass GPT(nn.Module):\n    def __init__(self, vocab_size, d_model, n_layers, n_heads, max_seq_len):\n        super().__init__()\n        \n        # 1. 词嵌入 (Token Embeddings)\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        \n        # 2. 位置嵌入 (Position Embeddings)\n        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n        \n        # 3. Transformer 解码器堆栈\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads) for _ in range(n_layers)\n        ])\n        \n        # 语言模型头（用于预训练）\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n        self.lm_head.weight = self.token_embedding.weight # 共享权重\n\n        # 生成因果掩码（只在预训练中使用）\n        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(max_seq_len, max_seq_len)).bool())\n\n    def forward(self, input_ids):\n        batch_size, seq_len = input_ids.shape\n        \n        # 论文公式 (1): h_0 = UW_e + W_p\n        token_emb = self.token_embedding(input_ids)\n        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)\n        position_emb = self.position_embedding(position_ids)\n        input_emb = token_emb + position_emb\n        \n        # 论文公式 (2): h_l = transformer_block(h_l-1)\n        x = input_emb\n        for block in self.transformer_blocks:\n            # 应用因果掩码\n            causal_mask = self.causal_mask[:seq_len, :seq_len]\n            x = block(x, causal_mask)\n        \n        # 论文公式 (3): P(u) = softmax(h_n * W_e^T)\n        logits = self.lm_head(x)\n        return logits\n\n# ----------------- 第2部分：无监督预训练 -----------------\n\ndef pretrain_gpt(model, dataloader, optimizer, num_epochs):\n    model.train()\n    lm_criterion = nn.CrossEntropyLoss()\n    \n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            input_ids = batch # 假设dataloader已经处理好了token ID\n            targets = input_ids[:, 1:] # 目标是下一个词\n            input_ids = input_ids[:, :-1] # 输入是除去最后一个词的序列\n\n            # 正向传播\n            logits = model(input_ids)\n            \n            # 计算因果语言建模损失（L1）\n            # logits.view(-1, logits.size(-1)) 将 (B, S, V) 展平为 (B*S, V)\n            # targets.view(-1) 将 (B, S) 展平为 (B*S)\n            loss = lm_criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n            \n            # 反向传播和优化\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n# ----------------- 第3部分：有监督微调 -----------------\n\ndef fine_tune_gpt(model, dataloader, optimizer, num_epochs, lambda_lm):\n    model.train()\n    task_criterion = nn.CrossEntropyLoss()\n    lm_criterion = nn.CrossEntropyLoss()\n    \n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            # 论文中 \"任务感知输入转换\" 的伪代码\n            # input_ids: [CLS_token, sentence_A_tokens, SEP_token, sentence_B_tokens, END_token]\n            # task_labels: [0, 1] (二分类)\n            input_ids, task_labels = batch \n            \n            # 在模型输出前添加分类头\n            # 论文图1 FFNs层\n            classification_head = nn.Linear(model.lm_head.in_features, task_labels.shape[1])\n            \n            # 正向传播，获得[CLS] token的表示\n            # [CLS] token 位于序列的开头，其输出表示融合了整个序列的信息\n            logits = model(input_ids)\n            cls_h_n = logits[:, 0, :] # 获得[CLS] token的最终隐藏状态\n            task_logits = classification_head(cls_h_n)\n\n            # 论文公式 (5): L2(C) = ...\n            task_loss = task_criterion(task_logits, task_labels)\n            \n            # 论文公式 (6): L3(C) = L2(C) + lambda * L1(C)\n            # 计算辅助语言模型损失（L1）\n            targets = input_ids[:, 1:]\n            lm_logits = logits[:, :-1, :]\n            lm_loss = lm_criterion(lm_logits.view(-1, lm_logits.size(-1)), targets.view(-1))\n            \n            total_loss = task_loss + lambda_lm * lm_loss\n            \n            # 反向传播和优化\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n            \n            print(f\"Epoch {epoch}, Total Loss: {total_loss.item()}\")","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}