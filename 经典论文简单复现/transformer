{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMj7Glga/Neys0A8gUnKFK9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["'''\n","总体的流程设计\n","分为编码块Encoder和解码块Decoder两部分\n","1. encoder\n","  1.1 tokenize + embdding\n","  1.2 multi-head attention\n","    1.2.1 attention\n","      给定x，投影到q,k,v\n","\n","  1.3 resident_netword+ layer_norm\n","  1.4 ffn\n","\n","2. decoder\n","...\n","\n","3. Transformer\n","...\n","'''"],"metadata":{"id":"LxSXSfggNdb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 最终版本\n","import torch\n","import torch.nn as nn\n","import math\n","\n","# 补全 Positional Encoding 函数\n","def positional_encoding(seq_len, d_model):\n","  \"\"\"\n","  Returns:\n","    torch.Tensor: 位置编码张量，形状为 (1, seq_len, d_model)。\n","  \"\"\"\n","  # 创建一个形状为 (seq_len, 1) 的位置张量\n","  pos = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n","  # 创建一个形状为 (d_model,) 的维度索引张量\n","  div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","  # 计算位置编码\n","  pe = torch.zeros(seq_len, d_model)\n","  pe[:, 0::2] = torch.sin(pos * div_term)\n","  pe[:, 1::2] = torch.cos(pos * div_term)\n","\n","  # 增加一个批次维度并返回\n","  return pe.unsqueeze(0)"],"metadata":{"id":"IVrcZyFzxN6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):\n","  def __init__(self, d_model, head_size):\n","    super().__init__()\n","    self.query = nn.Linear(d_model, head_size)\n","    self.key = nn.Linear(d_model, head_size)\n","    self.value = nn.Linear(d_model, head_size)\n","    self.head_size = head_size\n","\n","\n","  def forward(self, x, mask = None):\n","    # x: (batch_size, seq_len, d_model)\n","    q = self.query(x)\n","    k = self.key(x)\n","    v = self.value(x)\n","    # q, k, v: (batch_size, seq_len, d_model)\n","    scores = (q @ k.transpose(-2, -1))/ self.head_size**0.5\n","    if mask is not None:\n","      scores = scores.masked_fill(mask == 0, float('-inf'))\n","    # scores: (bs, sl, sl)\n","    scores = torch.softmax(scores, dim = -1)\n","    output = scores @ v\n","    # output: (bs, sl, d_model)\n","    return output\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_model, num_heads):\n","    super().__init__()\n","    self.head_size = d_model // num_heads\n","    self.mha_blocks = nn.ModuleList([Attention(d_model, self.head_size) for _ in range(num_heads)])\n","    self.proj = nn.Linear(d_model, d_model)\n","\n","  def forward(self, x):\n","    # NOTE: Your implementation is not efficient, but we keep your style\n","    # to demonstrate the correct logic.\n","    output_heads = [block(x) for block in self.mha_blocks]\n","    output = torch.cat(output_heads, dim=-1)\n","    output = self.proj(output)\n","    return output\n","\n"],"metadata":{"id":"AZ7lnDZp6ynK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CrossAttention(nn.Module):\n","  def __init__(self, d_model, head_size):\n","    super().__init__()\n","    self.query = nn.Linear(d_model, head_size)\n","    self.key = nn.Linear(d_model, head_size)\n","    self.value = nn.Linear(d_model, head_size)\n","    self.head_size = head_size\n","\n","  def forward(self, query_input, kv_input, mask=None):\n","    q = self.query(query_input)\n","    k = self.key(kv_input)\n","    v = self.value(kv_input)\n","\n","    scores = (q @ k.transpose(-2, -1)) / self.head_size**0.5\n","    if mask is not None:\n","      scores = scores.masked_fill(mask == 0, float('-inf'))\n","\n","    scores = torch.softmax(scores, dim=-1)\n","    output = scores @ v\n","    return output\n","\n","class MultiHeadCrossAttention(nn.Module):\n","  def __init__(self, d_model, num_heads):\n","    super().__init__()\n","    self.head_size = d_model // num_heads\n","    self.mha_blocks = nn.ModuleList([CrossAttention(d_model, self.head_size) for _ in range(num_heads)])\n","    self.proj = nn.Linear(d_model, d_model)\n","\n","  def forward(self, decoder_input, encoder_output):\n","    output_heads = [block(decoder_input, encoder_output) for block in self.mha_blocks]\n","    output = torch.cat(output_heads, dim=-1)\n","    output = self.proj(output)\n","    return output"],"metadata":{"id":"3F69Ma0htaN8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","  def __init__(self, d_model, d_ff):\n","    super().__init__()\n","    self.linear1 = nn.Linear(d_model, d_ff)\n","    self.linear2 = nn.Linear(d_ff, d_model)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, mha_output):\n","    output = self.linear2(self.relu(self.linear1(mha_output)))\n","    return output\n","\n","class EncoderBlock(nn.Module):\n","  def __init__(self, d_model, num_heads, d_ff):\n","    super().__init__()\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ff = FeedForward(d_model, d_ff)\n","    self.norm1 = nn.LayerNorm(d_model)\n","    self.norm2 = nn.LayerNorm(d_model)\n","\n","  def forward(self, x, mask=None):\n","    # 编码器自注意力通常不使用因果掩码，但可能需要用于padding的掩码。\n","    # 为了保持简单，我们假设不使用掩码。\n","    mha_output = self.norm1(self.mha(x) + x)\n","    ff_output = self.norm2(self.ff(mha_output) + mha_output)\n","    return ff_output"],"metadata":{"id":"QI9oOzKXte0G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MaskedMHA(nn.Module):\n","  def __init__(self, d_model, num_heads):\n","    super().__init__()\n","    self.head_size = d_model // num_heads\n","    self.mha_blocks = nn.ModuleList([Attention(d_model, self.head_size) for _ in range(num_heads)])\n","    self.proj = nn.Linear(d_model, d_model)\n","\n","\n","  def forward(self, x, mask=None):\n","    # 修复了掩码形状的错误，现在它能够被广播\n","    bs, sl, dm = x.shape\n","    if mask is None:\n","        mask = torch.tril(torch.ones(sl, sl)).unsqueeze(0).unsqueeze(0).to(x.device)\n","\n","    output_heads = [block(x, mask) for block in self.mha_blocks]\n","    output = torch.cat(output_heads, dim=-1)\n","    output = self.proj(output)\n","    return output\n","\n","class DecoderBlock(nn.Module):\n","  def __init__(self, d_model, num_heads, dff):\n","    super().__init__()\n","    self.masked_mha = MaskedMHA(d_model, num_heads)\n","    self.cross_mha = MultiHeadCrossAttention(d_model, num_heads)\n","    self.ff = FeedForward(d_model, dff)\n","    self.norm1 = nn.LayerNorm(d_model)\n","    self.norm2 = nn.LayerNorm(d_model)\n","    self.norm3 = nn.LayerNorm(d_model)\n","\n","  def forward(self, x, encoder_output):\n","    # x: Decoder的输入，(bs, decoder_sl, d_model)\n","    # encoder_output: Encoder的输出，(bs, encoder_sl, d_model)\n","\n","    # 第一个子层：掩码多头自注意力\n","    masked_output = self.norm1(self.masked_mha(x) + x)\n","\n","    # 第二个子层：多头交叉注意力 (Encoder-Decoder Attention)\n","    cross_output = self.norm2(self.cross_mha(masked_output, encoder_output) + masked_output)\n","\n","    # 第三个子层：前馈网络\n","    ff_output = self.norm3(self.ff(cross_output) + cross_output)\n","\n","    return ff_output\n","\n"],"metadata":{"id":"7KMPrW_LtmJm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","  def __init__(self, d_model, num_heads, dff, num_layers, vocab_size):\n","    super().__init__()\n","    self.encoder_stack = nn.ModuleList([EncoderBlock(d_model, num_heads, dff) for _ in range(num_layers)])\n","    self.decoder_stack = nn.ModuleList([DecoderBlock(d_model, num_heads, dff) for _ in range(num_layers)])\n","    self.embedding = nn.Embedding(vocab_size, d_model)\n","    self.final_linear = nn.Linear(d_model, vocab_size)\n","\n","  def forward(self, encoder_input, decoder_input):\n","    # 1. 对编码器输入进行词嵌入和位置编码\n","    encoder_input_emb = self.embedding(encoder_input)\n","    bs, enc_seq_len = encoder_input.shape\n","    encoder_input_emb = encoder_input_emb + positional_encoding(enc_seq_len, encoder_input_emb.shape[-1]).to(encoder_input_emb.device)\n","\n","    # 2. 依次通过N个编码器层\n","    encoder_output = encoder_input_emb\n","    for encoder_layer in self.encoder_stack:\n","      encoder_output = encoder_layer(encoder_output)\n","\n","    # 3. 对解码器输入进行词嵌入和位置编码\n","    decoder_input_emb = self.embedding(decoder_input)\n","    bs, dec_seq_len = decoder_input.shape\n","    decoder_input_emb = decoder_input_emb + positional_encoding(dec_seq_len, decoder_input_emb.shape[-1]).to(decoder_input_emb.device)\n","\n","    # 4. 依次通过N个解码器层\n","    decoder_output = decoder_input_emb\n","    for decoder_layer in self.decoder_stack:\n","      # 每一层解码器都需要接收上一层的输出和编码器的最终输出\n","      decoder_output = decoder_layer(decoder_output, encoder_output)\n","\n","    # 5. 最后通过线性层得到输出\n","    output = self.final_linear(decoder_output)\n","    # 返回logits\n","    return output\n"],"metadata":{"id":"ut8CoaULtmMC"},"execution_count":null,"outputs":[]}]}