{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5GGS4AZDoEKjr94/R5JVT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# RoPE\n","它的核心思想是：通过在自注意力机制的查询（Query, Q）和键（Key, K）向量中注入相对位置信息。 它不是简单地将位置信息加到词嵌入上，而是通过旋转的方式，使得点积注意力中的相关性自然地编码了相对位置。\n","1. 数学原理\n","假设我们有一个 d 维的特征向量 x，我们希望在其中注入位置 m 的信息。RoPE 构造了一个旋转操作 R m，使得：当计算两个向量 q 和 k 的点积时，如果它们分别处于位置 m 和 n，我们希望它们的相似性能够反映它们的相对位置 m−n。RoPE 的实现方式是让 Q 和 K 向量的每个二维子空间（或称“pair of dimensions”）进行旋转。\n","**一个核心概念**</br>\n","在复平面中，一个复数对模长为1的极坐标复数取模，就视为一个**“旋转”**了。\n","2. 操作流程\n","输入input_seqs[bsize, nheads, seq_len, head_d]</br>\n","生成旋转频率</br>\n","生成cos(p* theta)和sin(p* theta)</br>\n","3. 应用旋转\n","4. 旋转后的Q和K可以用于后续的注意力计算"],"metadata":{"id":"PIO-7I8xk2k8"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","\n","# (RotaryPositionalEmbedding 类代码同上，这里省略重复部分，假设它已经被定义)\n","\n","class RotaryPositionalEmbedding(nn.Module):\n","    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n","        super().__init__()\n","        if dim % 2 != 0:\n","            raise ValueError(f\"Embedding dimension 'dim' must be an even number, but got {dim}\")\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer(\"inv_freq\", inv_freq)\n","        t = torch.arange(max_seq_len, dtype=torch.float32)\n","        freqs = torch.einsum('i,j->ij', t, inv_freq)\n","        emb = torch.cat((freqs, freqs), dim=-1)\n","        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n","        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n","\n","    def forward(self, x: torch.Tensor, seq_len: int = None) -> torch.Tensor:\n","        current_seq_len = x.shape[-2] if seq_len is None else seq_len\n","        if current_seq_len > self.cos_cached.shape[-2]:\n","            raise ValueError(f\"Sequence length {current_seq_len} exceeds precomputed max_seq_len {self.cos_cached.shape[-2]}.\")\n","        cos = self.cos_cached[:, :, :current_seq_len, :].to(x.device)\n","        sin = self.sin_cached[:, :, :current_seq_len, :].to(x.device)\n","        x_rotated_half = torch.cat((-x[..., x.shape[-1]//2:], x[..., :x.shape[-1]//2]), dim=-1)\n","        rotated_x = x * cos + x_rotated_half * sin\n","        return rotated_x"],"metadata":{"id":"71VjQR_u9U4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4CHWFpjcnQO"},"outputs":[],"source":["# --- 样例数据集生成和演示 ---\n","\n","def demo_rope_with_data():\n","    # 1. 定义 Transformer Attention 层的相关参数\n","    batch_size = 4        # 批次大小\n","    num_heads = 12        # 注意力头的数量\n","    sequence_length = 64  # 输入序列的长度，例如一个句子有 64 个词元\n","    head_dim = 64         # 每个注意力头的维度 (d_model / num_heads)。必须是偶数。\n","\n","    # RoPE 的最大序列长度，通常设置得比实际使用长度大，以便泛化到更长序列\n","    # 如果实际序列长度超出此值，RoPE 可能需要重新计算或引发错误。\n","    rope_max_seq_len = 1024\n","    rope_base = 10000.0   # RoPE 的基数，影响旋转频率\n","\n","    # 2. 模拟生成 Query 和 Key 张量\n","    # 这些张量通常是经过线性投影后的结果，形状为 (batch_size, num_heads, sequence_length, head_dim)\n","    try:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f\"使用设备: {device}\")\n","        query_tensor = torch.randn(batch_size, num_heads, sequence_length, head_dim, dtype=torch.float32).to(device)\n","        key_tensor = torch.randn(batch_size, num_heads, sequence_length, head_dim, dtype=torch.float32).to(device)\n","        # Value tensor is not directly affected by RoPE, but we include it for context\n","        value_tensor = torch.randn(batch_size, num_heads, sequence_length, head_dim, dtype=torch.float32).to(device)\n","    except RuntimeError:\n","        device = torch.device(\"cpu\")\n","        print(f\"使用设备: {device}\")\n","        query_tensor = torch.randn(batch_size, num_heads, sequence_length, head_dim, dtype=torch.float32).to(device)\n","        key_tensor = torch.randn(batch_size, num_heads, sequence_length, head_dim, dtype=torch.float32).to(device)\n","        value_tensor = torch.randn(batch_size, num_heads, sequence_length, head_dim, dtype=torch.float32).to(device)\n","\n","    print(f\"\\n--- RoPE 层演示 ---\")\n","    print(f\"原始 Query 张量形状: {query_tensor.shape}\")\n","    print(f\"原始 Key 张量形状: {key_tensor.shape}\")\n","    print(f\"原始 Value 张量形状: {value_tensor.shape}\") # Value不受RoPE影响\n","    print(f\"输入数据类型: {query_tensor.dtype}\")\n","    print(f\"输入数据设备: {query_tensor.device}\")\n","\n","    # 3. 实例化 RoPE 层\n","    # RoPE 的 dim 参数对应于每个注意力头的维度 (head_dim)\n","    rope_layer = RotaryPositionalEmbedding(dim=head_dim, max_seq_len=rope_max_seq_len, base=rope_base).to(device)\n","    print(f\"\\nRoPE 层实例化完成:\")\n","    print(f\"  RoPE 维度 (head_dim): {head_dim}\")\n","    print(f\"  预计算最大序列长度: {rope_max_seq_len}\")\n","    print(f\"  基数 (base): {rope_base}\")\n","    print(f\"  cos_cached 形状: {rope_layer.cos_cached.shape}\")\n","    print(f\"  sin_cached 形状: {rope_layer.sin_cached.shape}\")\n","\n","    # 4. 应用 RoPE 到 Query 和 Key\n","    print(f\"\\n应用 RoPE 到 Query 和 Key 张量...\")\n","    query_with_rope = rope_layer(query_tensor)\n","    key_with_rope = rope_layer(key_tensor)\n","\n","    print(f\"应用 RoPE 后 Query 张量形状: {query_with_rope.shape}\")\n","    print(f\"应用 RoPE 后 Key 张量形状: {key_with_rope.shape}\")\n","    print(f\"输出数据类型: {query_with_rope.dtype}\")\n","    print(f\"输出数据设备: {query_with_rope.device}\")\n","\n","    # 5. 验证 RoPE 效果\n","    # 验证维度是否保持不变\n","    assert query_with_rope.shape == query_tensor.shape\n","    assert key_with_rope.shape == key_tensor.shape\n","    print(\"\\n验证: RoPE 处理后张量形状与原始张量形状一致。\")\n","\n","    # 验证值是否被旋转 (即不再与原始值完全相同)\n","    # 随机选择一个样本、一个头、一个位置，检查其向量变化\n","    sample_idx, head_idx, pos_idx = 0, 0, 0\n","    original_q_vec = query_tensor[sample_idx, head_idx, pos_idx, :]\n","    rotated_q_vec = query_with_rope[sample_idx, head_idx, pos_idx, :]\n","\n","    print(f\"\\n--- 特征向量旋转示例 (batch={sample_idx}, head={head_idx}, pos={pos_idx}) ---\")\n","    print(\"原始 Query 向量 (前5个维度):\")\n","    print(original_q_vec[:5].cpu().numpy())\n","    print(\"RoPE 处理后 Query 向量 (前5个维度):\")\n","    print(rotated_q_vec[:5].cpu().numpy())\n","\n","    # 检查向量是否确实被修改\n","    assert not torch.allclose(original_q_vec, rotated_q_vec, atol=1e-6), \"错误: 向量未被旋转！\"\n","    print(\"\\n验证: 向量值已成功被 RoPE 旋转。\")\n","\n","    # 6. 模拟注意力分数计算（ RoPE 的效果体现在这里）\n","    # 理论上，(Q @ K.T) 经过 RoPE 后会编码相对位置信息\n","    # 这里只是一个概念性演示，实际注意力机制会更复杂\n","    print(f\"\\n--- 模拟注意力分数计算 ---\")\n","    # 为了简化演示，我们只计算一个样本的 Query 和 Key 之间的点积\n","    # Q: (num_heads, seq_len, head_dim)\n","    # K.T: (num_heads, head_dim, seq_len)\n","    # score: (num_heads, seq_len, seq_len)\n","\n","    # 原始 Q 和 K 的注意力分数 (简化)\n","    raw_scores = torch.matmul(query_tensor[0], key_tensor[0].transpose(-2, -1))\n","    print(f\"原始注意力分数 (第一个batch, 第一个head, 前5x5):\")\n","    print(raw_scores[0, :5, :5].cpu().numpy())\n","\n","    # 经过 RoPE 处理后的 Q 和 K 的注意力分数\n","    rope_scores = torch.matmul(query_with_rope[0], key_with_rope[0].transpose(-2, -1))\n","    print(f\"\\nRoPE 处理后注意力分数 (第一个batch, 第一个head, 前5x5):\")\n","    print(rope_scores[0, :5, :5].cpu().numpy())\n","\n","    # 观察：对角线（相对位置0）和远离对角线（相对位置非0）的数值模式会因RoPE而改变。\n","    # 这种变化使得模型能够根据相对位置调整注意力。\n","\n","# 运行演示\n","demo_rope_with_data()"]},{"cell_type":"code","source":[],"metadata":{"id":"0usVQ0CTk2Q1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JvekPyvOk2TC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jwpUP-ePk2Vk"},"execution_count":null,"outputs":[]}]}