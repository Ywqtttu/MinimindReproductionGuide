{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPL6oHAx96JGVnPYaIojQ4u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DnDfuXBrICmp"},"outputs":[],"source":["import torch\n","from torch import optim, nn\n","\n","\n","# 定义Lora网络结构\n","class LoRA(nn.Module):\n","    def __init__(self, in_features, out_features, rank):\n","        super().__init__()\n","        self.rank = rank  # LoRA的秩（rank），控制低秩矩阵的大小\n","        self.A = nn.Linear(in_features, rank, bias=False)  # 低秩矩阵A\n","        self.B = nn.Linear(rank, out_features, bias=False)  # 低秩矩阵B\n","        # 矩阵A高斯初始化\n","        self.A.weight.data.normal_(mean=0.0, std=0.02)\n","        # 矩阵B全0初始化\n","        self.B.weight.data.zero_()\n","\n","    def forward(self, x):\n","        return self.B(self.A(x))\n","\n","\n","def apply_lora(model, rank=8):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:\n","            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank).to(model.device)\n","            setattr(module, \"lora\", lora)\n","            original_forward = module.forward\n","\n","            # 显式绑定\n","            def forward_with_lora(x, layer1=original_forward, layer2=lora):\n","                return layer1(x) + layer2(x)\n","\n","            module.forward = forward_with_lora\n","\n","\n","def load_lora(model, path):\n","    state_dict = torch.load(path, map_location=model.device)\n","    for name, module in model.named_modules():\n","        if hasattr(module, 'lora'):\n","            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}\n","            module.lora.load_state_dict(lora_state)\n","\n","\n","def save_lora(model, path):\n","    state_dict = {}\n","    for name, module in model.named_modules():\n","        if hasattr(module, 'lora'):\n","            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()}\n","            state_dict.update(lora_state)\n","    torch.save(state_dict, path)\n"]}]}