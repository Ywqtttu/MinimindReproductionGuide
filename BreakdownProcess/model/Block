{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+/ckhSxRGmtdxlFfx6yh8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer Block\n","Transformer Encoder Block 是 Transformer 模型编码器部分的核心构建块。它的作用是将输入的序列表示（例如，词嵌入与位置编码的结合）进一步提炼和转换，捕捉序列中不同位置之间的复杂关系，并输出一个包含更丰富上下文信息的相同形状的序列。多个 Encoder Block 通常堆叠在一起以增加模型的深度。\n","1. 组成部分：\n","一个典型的 Encoder Block 通常包含：\n","\n","  多头自注意力机制 (Multi-Head Self-Attention, MHSA)</br>\n","残差连接 (Residual Connection)</br>\n","层归一化 (Layer Normalization, LN) 或 RMSNorm</br>\n","前馈网络 (Feed-Forward Network, FFN)\n"],"metadata":{"id":"DbeGtmnYBVa9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4CHWFpjcnQO"},"outputs":[],"source":["'''\n","笔者认为，这个Block有点像MLP——把已经写好的各个层封装起来，\n","然后规定一个传播过程，其实他本身并没有特别的功能，\n","只是提供了张量输出和被其他层处理的逻辑（主要是先后顺序），\n","而具体的计算是依赖被封装的层实现的。\n","同时封装好的Block也可以很方便地堆叠起来进行多次计算，\n","增加网络的深度从而提升模型性能\n","因此这里偷个懒，就不费时间重复造轮子了。\n","'''"]}]}