{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMRvWp4ArzHnjGrf191J6Qa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# RMSNorm\n","核心思想：\n","RMSNorm (Root Mean Square Normalization) 是一种归一化技术，其主要思想是对输入张量的每个元素除以其均方根 (RMS)，以达到稳定神经网络训练的目的。与常见的 Layer Normalization 不同，RMSNorm 不会减去均值，这意味着它保留了特征的“平移不变性”（re-centering invariance），更侧重于对特征的**幅度（magnitude）**进行归一化。</br>\n","1. 计算均方根\n","2. 归一化：也就是除以均方根\n","3. （可选）对张量进行缩放，该参数weight可被学习\n"],"metadata":{"id":"YbNe6ZuxdMwp"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"x4CHWFpjcnQO","executionInfo":{"status":"ok","timestamp":1748597164733,"user_tz":-480,"elapsed":8,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["class RMSNorm(nn.Module):\n","  def __init__(self, dim, eps = 1e-6):\n","    super().__init__()\n","    self.eps = eps\n","    self.dim = dim\n","    self.weight = nn.Parameter(torch.ones(dim))\n","\n","\n","  def _norm(self, x):\n","    rms_x = torch.rsqrt(x.pow(2).mean(-1, keepdim = True) + self.eps)\n","    return x * rms_x\n","\n","\n","  def forward(self, x):\n","    return self._norm(x.float()).type_as(x) * self.weight"],"metadata":{"id":"uXrm7V2XdMZR","executionInfo":{"status":"ok","timestamp":1748597164757,"user_tz":-480,"elapsed":19,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# --- 验证 RMSNorm 的正确性 ---\n","def check_rms_norm():\n","    # 创建一个随机输入张量\n","    batch_size = 2\n","    seq_len = 5\n","    feature_dim = 4\n","    x = torch.randn(batch_size, seq_len, feature_dim)\n","\n","    # 初始化 RMSNorm 层\n","    rms_norm_layer = RMSNorm(dim=feature_dim)\n","\n","    # 进行前向传播\n","    output = rms_norm_layer(x)\n","\n","    print(\"--- RMSNorm 验证 ---\")\n","    print(f\"输入张量 x 形状: {x.shape}\")\n","    print(f\"输入张量 x 数据类型: {x.dtype}\")\n","    print(f\"输入张量 x 设备: {x.device}\")\n","    print(\"\\n原始输入 x:\")\n","    print(x)\n","\n","    print(\"\\nRMSNorm 归一化后的输出:\")\n","    print(output)\n","    print(f\"输出张量 output 形状: {output.shape}\")\n","    print(f\"输出张量 output 数据类型: {output.dtype}\")\n","    print(f\"输出张量 output 设备: {output.device}\")\n","\n","    # 验证归一化效果：\n","    # 计算输出的均方根，应该接近 1 (在考虑 gamma 缩放后)。\n","    # 如果 gamma 是全1，那么每个特征向量的 RMS 应该接近 1。\n","    # 我们可以手动计算一下：\n","    # 移除 gamma 的影响，即 output / weight\n","    output_without_gamma = output / rms_norm_layer.weight\n","    rms_of_output = torch.sqrt((output_without_gamma.pow(2).mean(-1, keepdim=True)))\n","    print(\"\\n每个归一化后特征向量的 RMS (不含gamma):\")\n","    print(rms_of_output)\n","    # 期望这些值都接近 1.0\n","\n","    # 也可以测试混合精度\n","    x_fp16 = x.half() # 转换为 float16\n","    output_fp16 = rms_norm_layer(x_fp16)\n","    print(\"\\n--- 混合精度 (float16) 测试 ---\")\n","    print(f\"输入张量 x_fp16 数据类型: {x_fp16.dtype}\")\n","    print(f\"输出张量 output_fp16 数据类型: {output_fp16.dtype}\")\n","    print(f\"输出张量 output_fp16 形状: {output_fp16.shape}\")\n","    print(\"\\nRMSNorm 归一化后的 float16 输出:\")\n","    print(output_fp16)"],"metadata":{"id":"Kjyi9cVEdMfU","executionInfo":{"status":"ok","timestamp":1748597164768,"user_tz":-480,"elapsed":9,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["check_rms_norm()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wqkygvUUdMhW","executionInfo":{"status":"ok","timestamp":1748597165005,"user_tz":-480,"elapsed":238,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"a1ee1c30-081e-45c0-e6cf-c9c1b58e49c0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--- RMSNorm 验证 ---\n","输入张量 x 形状: torch.Size([2, 5, 4])\n","输入张量 x 数据类型: torch.float32\n","输入张量 x 设备: cpu\n","\n","原始输入 x:\n","tensor([[[ 1.3819, -0.8070, -1.9535,  1.4018],\n","         [ 0.0659,  1.1150,  0.1526,  0.0918],\n","         [ 0.0055, -0.4674,  0.0471, -0.1710],\n","         [ 1.6314,  0.0165,  0.1086,  1.0166],\n","         [ 0.3424, -0.0889,  0.4060, -0.8189]],\n","\n","        [[-0.6987, -0.4687, -0.7417, -0.2912],\n","         [ 0.3796, -0.9872, -1.0510,  1.3867],\n","         [-1.4448, -1.2028,  0.6512, -0.4053],\n","         [-2.1989, -1.0851, -2.0173, -0.0086],\n","         [-0.1549, -0.0707,  0.7981,  0.2502]]])\n","\n","RMSNorm 归一化后的输出:\n","tensor([[[ 0.9569, -0.5588, -1.3527,  0.9707],\n","         [ 0.1165,  1.9716,  0.2698,  0.1624],\n","         [ 0.0221, -1.8698,  0.1885, -0.6839],\n","         [ 1.6946,  0.0172,  0.1128,  1.0560],\n","         [ 0.6986, -0.1813,  0.8285, -1.6711]],\n","\n","        [[-1.2059, -0.8090, -1.2801, -0.5026],\n","         [ 0.3728, -0.9696, -1.0323,  1.3620],\n","         [-1.4232, -1.1848,  0.6415, -0.3993],\n","         [-1.3850, -0.6835, -1.2706, -0.0054],\n","         [-0.3629, -0.1656,  1.8701,  0.5863]]], grad_fn=<MulBackward0>)\n","输出张量 output 形状: torch.Size([2, 5, 4])\n","输出张量 output 数据类型: torch.float32\n","输出张量 output 设备: cpu\n","\n","每个归一化后特征向量的 RMS (不含gamma):\n","tensor([[[1.0000],\n","         [1.0000],\n","         [1.0000],\n","         [1.0000],\n","         [1.0000]],\n","\n","        [[1.0000],\n","         [1.0000],\n","         [1.0000],\n","         [1.0000],\n","         [1.0000]]], grad_fn=<SqrtBackward0>)\n","\n","--- 混合精度 (float16) 测试 ---\n","输入张量 x_fp16 数据类型: torch.float16\n","输出张量 output_fp16 数据类型: torch.float32\n","输出张量 output_fp16 形状: torch.Size([2, 5, 4])\n","\n","RMSNorm 归一化后的 float16 输出:\n","tensor([[[ 0.9570, -0.5591, -1.3525,  0.9707],\n","         [ 0.1165,  1.9717,  0.2698,  0.1624],\n","         [ 0.0221, -1.8701,  0.1885, -0.6841],\n","         [ 1.6943,  0.0172,  0.1129,  1.0557],\n","         [ 0.6987, -0.1814,  0.8286, -1.6709]],\n","\n","        [[-1.2061, -0.8091, -1.2803, -0.5024],\n","         [ 0.3728, -0.9697, -1.0322,  1.3623],\n","         [-1.4229, -1.1855,  0.6416, -0.3992],\n","         [-1.3848, -0.6831, -1.2705, -0.0054],\n","         [-0.3628, -0.1655,  1.8701,  0.5864]]], grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"k2wS1-pRdMj0","executionInfo":{"status":"ok","timestamp":1748597165013,"user_tz":-480,"elapsed":1,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":6,"outputs":[]}]}