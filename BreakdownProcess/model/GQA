{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP93vspRy/6YxK++LJIvfPW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GQA 分组注意力机制\n","GQA (Grouped Query Attention) 的提出是为了在保持 MHA 大部分性能优势的同时，显著减少 KV Cache 的内存占用和推理时的计算量。\n","1. 核心思想：让多个查询头共享一组键值头\n","在 MHA 中，每个 Query 头 (Q\n","i\n","​\n"," ) 都有自己独立的 Key 头 (K\n","i\n","​\n"," ) 和 Value 头 (V\n","i\n","​\n"," )。GQA 打破了这种一对一的对应关系，它将 N\n","q\n","​\n","  个 Query 头分组，每组 Query 头共享 N\n","k\n","​\n","  个 Key 头和 N\n","v\n","​\n","  个 Value 头，其中 N\n","q\n","​\n"," ≥N\n","k\n","​\n"," =N\n","v\n","​\n"," 。最常见的设置是 N\n","k\n","​\n"," =N\n","v\n","​\n"," 。\n","\n","当 N\n","q\n","​\n"," =N\n","k\n","​\n"," =N\n","v\n","​\n","  时，GQA 退化为标准的 Multi-Head Attention (MHA)。\n","当 N\n","k\n","​\n"," =N\n","v\n","​\n"," =1 时，GQA 退化为 Multi-Query Attention (MQA)。MQA 是 GQA 的一个特例，它让所有 Query 头共享唯一的一个 Key 头和一个 Value 头。\n","GQA 介于 MHA 和 MQA 之间，提供了一个在性能和效率之间进行权衡的选项。\n","2. 结构和运行原理\n","输入： 与 MHA 类似，输入是一个特征张量 X（通常是经过层归一化后的）。\n","线性投影： X 经过三个独立的线性层，分别投影得到 Query (Q)、Key (K) 和 Value (V)。\n","Q 的投影维度仍是 D\n","model\n","​\n"," ，然后被拆分成 N\n","q\n","​\n","  个 Query 头。\n","K 和 V 的投影维度则被设计为 N\n","k\n","​\n"," ⋅D\n","h\n","​\n","  和 N\n","v\n","​\n"," ⋅D\n","h\n","​\n"," （其中 D\n","h\n","​\n","  是每个头的维度），然后分别拆分成 N\n","k\n","​\n","  个 Key 头和 N\n","v\n","​\n","  个 Value 头。这里的关键是 N\n","k\n","​\n","  和 N\n","v\n","​\n","  小于或等于 N\n","q\n","​\n"," 。\n","分组： N\n","q\n","​\n","  个 Query 头被划分为 N\n","q\n","​\n"," /N\n","k\n","​\n","  个组（假设 N\n","q\n","​\n","  能被 N\n","k\n","​\n","  整除）。每个组内的 Query 头共享同一个 Key 头和 Value 头。"],"metadata":{"id":"hfhK8JQKD63L"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4CHWFpjcnQO"},"outputs":[],"source":["'''\n","结构流程\n","1. 输入X\n","2. 投影X到Q, K, V\n","3. 给K, V头分组，几个Q头共享一组K,V头（这步实现比较难理解）\n","\n","q_proj(x)\n","k_proj(x)\n","v_proj(x)\n","\n","q -> (bsize, num_query_heads, seq_len, head_dim)\n","k -> (bsize, num_kv_heads, seq_len, head_dim)\n","v -> (bsize, num_kv_heads, seq_len, head_dim)\n","\n","假如使用RoPE，在这里就要对k和v进行操作\n","K/V 头的复制/广播：通过repeat_interleave或repeat操作，复制k和v头，使得k, v达到上述的维度。\n","缩放点积注意力：attn_scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n","加权和：attn_output = torch.matmul(attn_weights, v)\n","拼接：attn_output 重塑回 (batch_size, sequence_length, model_dim)。\n","'''"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","\n","# --- 辅助模块定义 (同之前Encoder/Decoder Block中使用的) ---\n","\n","# 1. Rotary Positional Embedding (RoPE) helper functions and class\n","# 这部分代码与之前提供的一致，用于在Q和K向量上应用旋转位置编码。\n","def apply_rotary_pos_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n","    # x: (..., seq_len, head_dim)\n","    # cos, sin: (1, 1, seq_len, head_dim) (or compatible for broadcasting)\n","    x0, x1 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n","    rotated_x0 = x0 * cos - x1 * sin\n","    rotated_x1 = x0 * sin + x1 * cos\n","    return torch.cat((rotated_x0, rotated_x1), dim=-1)\n","\n","class RotaryPositionalEmbeddingPrecomputed(nn.Module):\n","    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n","        super().__init__()\n","        if dim % 2 != 0:\n","            raise ValueError(f\"Embedding dimension 'dim' must be an even number, but got {dim}\")\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer(\"inv_freq\", inv_freq)\n","        t = torch.arange(max_seq_len, dtype=torch.float32)\n","        freqs = torch.einsum('i,j->ij', t, inv_freq)\n","        emb = torch.cat((freqs, freqs), dim=-1)\n","        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n","        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n","\n","    def forward(self, seq_len: int, device: torch.device, dtype: torch.dtype) -> tuple[torch.Tensor, torch.Tensor]:\n","        if seq_len > self.cos_cached.shape[-2]:\n","            raise ValueError(f\"Sequence length {seq_len} exceeds precomputed max_seq_len {self.cos_cached.shape[-2]}.\")\n","        return self.cos_cached[:, :, :seq_len, :].to(device=device, dtype=dtype), \\\n","               self.sin_cached[:, :, :seq_len, :].to(device=device, dtype=dtype)"],"metadata":{"id":"axOPbeYFR7Ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- GQA (Grouped Query Attention) Implementation ---\n","class GroupedQueryAttention(nn.Module):\n","    \"\"\"\n","    Grouped Query Attention (GQA) implementation.\n","    This module performs attention where multiple Query heads share the same Key/Value heads.\n","    \"\"\"\n","    def __init__(self, model_dim: int, num_query_heads: int, num_kv_heads: int,\n","                 dropout_rate: float = 0.1, use_rope: bool = True):\n","        super().__init__()\n","\n","        # Validate head configuration\n","        if model_dim % num_query_heads != 0:\n","            raise ValueError(f\"model_dim ({model_dim}) must be divisible by num_query_heads ({num_query_heads})\")\n","        if num_query_heads % num_kv_heads != 0:\n","            raise ValueError(f\"num_query_heads ({num_query_heads}) must be divisible by num_kv_heads ({num_kv_heads})\")\n","        if num_kv_heads > num_query_heads:\n","            raise ValueError(f\"num_kv_heads ({num_kv_heads}) cannot be greater than num_query_heads ({num_query_heads})\")\n","\n","        self.model_dim = model_dim\n","        self.num_query_heads = num_query_heads\n","        self.num_kv_heads = num_kv_heads\n","        self.head_dim = model_dim // num_query_heads # Query head_dim determines the head_dim for all\n","\n","        # The factor by which KV heads are repeated for Query heads\n","        self.kv_group_size = self.num_query_heads // self.num_kv_heads\n","\n","        # Linear projections for Q, K, V\n","        # Q projects to model_dim (num_query_heads * head_dim)\n","        # K and V project to (num_kv_heads * head_dim), which is smaller if num_kv_heads < num_query_heads\n","        # q -> (bsize, num_query_heads, seq_len, head_dim)\n","        # k -> (bsize, num_kv_heads, seq_len, head_dim)\n","        # v -> (bsize, num_kv_heads, seq_len, head_dim)\n","        self.q_proj = nn.Linear(model_dim, self.num_query_heads * self.head_dim, bias=False)\n","        self.k_proj = nn.Linear(model_dim, self.num_kv_heads * self.head_dim, bias=False)\n","        self.v_proj = nn.Linear(model_dim, self.num_kv_heads * self.head_dim, bias=False)\n","\n","        # Output projection layer\n","        self.out_proj = nn.Linear(self.num_query_heads * self.head_dim, model_dim, bias=False)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        # RoPE setup\n","        self.use_rope = use_rope\n","        if use_rope:\n","            self.rope = RotaryPositionalEmbeddingPrecomputed(self.head_dim)\n","\n","        # Scaling factor for attention scores\n","        self.scale = math.sqrt(self.head_dim)\n","\n","    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n","        \"\"\"\n","        Performs Grouped Query Attention.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor, typically from a previous layer.\n","                              Shape: (batch_size, sequence_length, model_dim)\n","            attention_mask (torch.Tensor, optional): Mask to prevent attention to padded tokens\n","                                                    or future tokens (causal mask).\n","                                                    Shape: (batch_size, 1, seq_len, seq_len) for causal mask\n","                                                    or (batch_size, 1, 1, seq_len) for padding mask (broadcastable).\n","                                                    Mask should be True for valid positions, False for masked.\n","\n","        Returns:\n","            torch.Tensor: Output of the GQA layer.\n","                          Shape: (batch_size, sequence_length, model_dim)\n","        \"\"\"\n","        batch_size, seq_len, _ = x.shape\n","\n","        # 1. Linear projections\n","        # q: (B, S, D_model) -> (B, S, num_query_heads * head_dim)\n","        # k, v: (B, S, D_model) -> (B, S, num_kv_heads * head_dim)\n","        q = self.q_proj(x)\n","        k = self.k_proj(x)\n","        v = self.v_proj(x)\n","\n","        # 2. Reshape to multi-head format and transpose\n","        # q: (B, S, num_query_heads * head_dim) -> (B, num_query_heads, S, head_dim)\n","        # k, v: (B, S, num_kv_heads * head_dim) -> (B, num_kv_heads, S, head_dim)\n","        q = q.view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n","        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n","        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n","\n","        # 3. Apply RoPE if enabled\n","        if self.use_rope:\n","            # RoPE is applied based on the sequence length and head dimension\n","            cos, sin = self.rope(seq_len=seq_len, device=x.device, dtype=x.dtype)\n","            q = apply_rotary_pos_emb(q, cos, sin)\n","            k = apply_rotary_pos_emb(k, cos, sin)\n","\n","        # 4. Repeat K and V heads to match the number of Query heads\n","        # This is the core of GQA when num_kv_heads < num_query_heads\n","        if self.kv_group_size > 1:\n","            # k: (B, num_kv_heads, S, head_dim) -> (B, num_query_heads, S, head_dim)\n","            k = k.repeat_interleave(self.kv_group_size, dim=1)\n","            v = v.repeat_interleave(self.kv_group_size, dim=1)\n","\n","        # 5. Scaled Dot-Product Attention calculation\n","        # attn_scores: (B, num_query_heads, S, S)\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n","\n","        # Apply attention mask\n","        if attention_mask is not None:\n","            # Ensure mask is broadcastable (B, 1, S, S) for (B, H, S, S) scores\n","            # Or handle various mask shapes.\n","            # Convert boolean mask (False means masked) to float mask (-inf for masked)\n","            # A common mask shape is (B, 1, S_q, S_k)\n","            attn_scores = attn_scores.masked_fill(attention_mask == 0, float('-inf'))\n","\n","\n","        attn_weights = torch.softmax(attn_scores, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # 6. Weighted sum of Values\n","        # attn_output: (B, num_query_heads, S, head_dim)\n","        attn_output = torch.matmul(attn_weights, v)\n","\n","        # 7. Concatenate heads and final linear projection\n","        # (B, num_query_heads, S, head_dim) -> (B, S, num_query_heads, head_dim) -> (B, S, model_dim)\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.model_dim)\n","        output = self.out_proj(attn_output)\n","\n","        return output"],"metadata":{"id":"TQP2FhDtSB8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 样例数据集生成和代码演示 (GQA) ---\n","\n","def demo_gqa_attention():\n","    # 1. 定义模型参数\n","    batch_size = 2          # 批次大小\n","    sequence_length = 128   # 序列长度\n","    model_dim = 768         # 模型的隐藏层维度 (d_model)\n","    num_query_heads = 12    # Query 头的数量 (例如 Llama 2 7B)\n","    num_kv_heads = 4        # Key/Value 头的数量 (GQA 的关键参数，num_kv_heads < num_query_heads)\n","    dropout_rate = 0.1      # Dropout 比率\n","    use_rope = True         # 是否使用 RoPE\n","\n","    # 2. 生成样例输入数据\n","    # 模拟经过层归一化后的输入张量，例如来自前一个解码器块或嵌入层\n","    try:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f\"使用设备: {device}\")\n","        input_data = torch.randn(batch_size, sequence_length, model_dim, dtype=torch.float32).to(device)\n","    except RuntimeError:\n","        device = torch.device(\"cpu\")\n","        print(f\"使用设备: {device}\")\n","        input_data = torch.randn(batch_size, sequence_length, model_dim, dtype=torch.float32).to(device)\n","\n","    # 3. 生成注意力掩码 (模拟 Transformer 中的 causal mask)\n","    # 对于自回归生成，解码器需要一个 look-ahead mask，防止关注未来信息\n","    # shape: (seq_len, seq_len)\n","    causal_mask = torch.ones(sequence_length, sequence_length, dtype=torch.bool).triu(diagonal=1).to(device)\n","    # Convert to a format that can be broadcasted (B, 1, S, S)\n","    # False where attention is allowed, True where it should be masked\n","    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, num_query_heads, -1, -1)\n","\n","    # Combined with padding mask (if any), a full attention mask would be:\n","    # attention_mask = causal_mask | padding_mask.unsqueeze(1).unsqueeze(2) (if padding_mask is (B, S))\n","    # For this demo, let's just use the causal_mask as the primary mask.\n","    attention_mask_for_gqa = (causal_mask == False) # Masking in GQA assumes False means mask\n","\n","    print(f\"\\n--- Grouped Query Attention (GQA) 演示 ---\")\n","    print(f\"输入数据形状: {input_data.shape}\")\n","    print(f\"输入数据类型: {input_data.dtype}\")\n","    print(f\"输入数据设备: {input_data.device}\")\n","    print(f\"注意力掩码形状: {attention_mask_for_gqa.shape}\")\n","\n","    # 4. 实例化 GQA 层\n","    gqa_layer = GroupedQueryAttention(\n","        model_dim=model_dim,\n","        num_query_heads=num_query_heads,\n","        num_kv_heads=num_kv_heads,\n","        dropout_rate=dropout_rate,\n","        use_rope=use_rope\n","    ).to(device)\n","\n","    print(f\"\\nGQA 层实例化完成:\")\n","    print(f\"  模型维度 (d_model): {model_dim}\")\n","    print(f\"  Query 头数: {num_query_heads}\")\n","    print(f\"  Key/Value 头数: {num_kv_heads}\")\n","    print(f\"  KV Group Size: {gqa_layer.kv_group_size}\")\n","    print(f\"  每个头的维度 (head_dim): {gqa_layer.head_dim}\")\n","    print(f\"  是否使用 RoPE: {use_rope}\")\n","    print(f\"  层设备: {next(gqa_layer.parameters()).device}\")\n","\n","    # 5. 执行前向传播\n","    print(f\"\\n开始前向传播...\")\n","    output_data = gqa_layer(input_data, attention_mask=attention_mask_for_gqa)\n","\n","    print(f\"前向传播完成。\")\n","    print(f\"GQA 层输出数据形状: {output_data.shape}\")\n","    print(f\"GQA 层输出数据类型: {output_data.dtype}\")\n","    print(f\"GQA 层输出数据设备: {output_data.device}\")\n","\n","    # 6. 验证输出形状\n","    assert output_data.shape == input_data.shape, \"错误: 输出张量形状与输入张量形状不匹配！\"\n","    print(\"\\n验证: 输出张量形状与输入张量形状匹配。\")\n","\n","    # 7. 验证输出值是否被修改 (确保模型在工作)\n","    assert not torch.allclose(input_data[0, 0, :], output_data[0, 0, :]), \"错误: 输出数据与输入数据完全相同，模型可能未进行有效处理！\"\n","    print(\"验证: 输出数据已被模型处理。\")\n","\n","    # --- 关键点验证：KV Cache 大小 ---\n","    # 在推理时，KV Cache 会存储 Key 和 Value 矩阵。\n","    # 对于一个生成步骤，KV Cache 的大小是 (batch_size, num_kv_heads, current_seq_len, head_dim)\n","    # 标准 MHA (num_kv_heads = num_query_heads) 的 KV Cache 大小是：\n","    mha_kv_cache_size = batch_size * num_query_heads * sequence_length * gqa_layer.head_dim * 2 # *2 for K and V\n","    print(f\"\\n理论上 MHA 的 KV Cache 元素总数 (K+V): {mha_kv_cache_size}\")\n","\n","    # GQA 的 KV Cache 大小是：\n","    gqa_kv_cache_size = batch_size * num_kv_heads * sequence_length * gqa_layer.head_dim * 2 # *2 for K and V\n","    print(f\"理论上 GQA 的 KV Cache 元素总数 (K+V): {gqa_kv_cache_size}\")\n","\n","    reduction_factor = mha_kv_cache_size / gqa_kv_cache_size\n","    print(f\"GQA 相比 MHA 减少 KV Cache 的倍数: {reduction_factor:.2f}x\")\n","    print(f\"(这正是 GQA 的主要优势所在，尤其在推理长序列时大幅节约显存)\")\n"],"metadata":{"id":"AmY7H0CeD6mh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 运行 GQA 演示\n","demo_gqa_attention()"],"metadata":{"id":"wJfPqVqND6ov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","# 示例 4: 在 GQA/MQA 中 K/V 头的复制\n","# 假设我们有 4 个 KV 头，需要复制以匹配 12 个 Query 头\n","batch_size = 2\n","num_kv_heads = 4\n","sequence_length = 5\n","head_dim = 64\n","\n","# K 张量，形状为 (batch_size, num_kv_heads, sequence_length, head_dim)\n","kv_tensor = torch.randn(batch_size, num_kv_heads, sequence_length, head_dim)\n","print(f\"\\n示例 4 (GQA/MQA 复制):\")\n","print(f\"原始 KV 张量形状: {kv_tensor.shape}\")\n","\n","num_query_heads = 12\n","kv_group_size = num_query_heads // num_kv_heads # 12 // 4 = 3\n","\n","# 沿着第 1 维 (头维度) 重复 kv_group_size 次\n","repeated_kv_tensor = torch.repeat_interleave(kv_tensor, repeats=kv_group_size, dim=1)\n","print(f\"重复后的 KV 张量形状: {repeated_kv_tensor.shape}\")\n","# 预期形状：(batch_size, num_query_heads, sequence_length, head_dim)\n","# 即 (2, 12, 5, 64)\n","\n","assert repeated_kv_tensor.shape[1] == num_query_heads"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkZvX1i3mD4f","executionInfo":{"status":"ok","timestamp":1748699230719,"user_tz":-480,"elapsed":104,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"7da504de-280e-4e12-e2cf-ce8b7e01c4bc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","示例 4 (GQA/MQA 复制):\n","原始 KV 张量形状: torch.Size([2, 4, 5, 64])\n","重复后的 KV 张量形状: torch.Size([2, 12, 5, 64])\n"]}]}]}