{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOPW0FG/d86yH52+VPudoqK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **写在最前面**\n","【对这个项目的介绍和对原作者的赞颂】</br>\n","【在这个项目中的收获】</br>\n","【为什么要做这样一个复现指南】</br>\n","某天笔者第n次翻阅原项目时，发现好像已经提前有老哥做了MiniMind的分解仓库了，大惊，上去大致看了一部分，写得非常详细，甚至是从tokenizer开始分析的。</br>\n","对于一个从0开始入门LLM的人来说，这样自然最合适不过，但对于笔者这样稍微有些基础的学生来说，未免过于繁琐冗长。</br>\n","同时，自下而上地拆解一个大模型容易使得读者一下全泡在底层的Module里而难以快速“入门”这个大模型的结构和运行机制，也和笔者最开始设计的自上而下的分解流程有所不同，因此笔者认为笔者的工作仍有必要，对笔者自己，我完整、独立地拆解、分析、复现了一个轻量但性能足够的大模型，极大地增进了对大模型的理解；对于其他读者，我也开源了另一条复现的路径，也算有所为于社区。\n"],"metadata":{"id":"IGVKESi-KEV8"}},{"cell_type":"markdown","source":["首先，我们从项目的整体结构入手。</br>\n","\n","图中几个文件夹中，排除掉存放数据集相关的dataset文件夹和存放文档插图的images文件夹以及负责命令行交互相关的scripts文件夹，主要的文件夹就只有\"out\", \"trainer\", \"model\"这三位，而\"out\"目前又是空的，那么就只剩下\"model\"和\"trainer\"两份文件夹需要我们重点研究</br>\n","\n","点开子目录，除开__init__.py以外，可以看到model目录下共有**model_lora.py**, **model_minimind.py**, **tokenizer.json**, **tokenizer_config.json**几个文件，翻阅原项目的readme文档可以知道，_lora是为后续模型微调准备的，而稍后对trainer的分析我们可以知道，两个tokenizer相关的文件，是为了后面实例化tokenizer准备的，只有**minimind_model.py**是需要我们重点攻克的主模型。</br>\n","\n","接着看到trainer目录，下有好几个训练脚本，一时缭乱难以自辨，我们回到readme.md的“主要训练步骤”一节，可以知道，训练大模型最重要的预训练部分的脚本也就是**train_pretrain.py**这一位，其余的微调训练之类的代码，我们可以暂时先放到一边。</br>\n","由是，现在的目标相当清晰了，我们就是要首先攻破**\"train_pretrain.py\"**和**\"minimind_model.py\"**这两部分代码。"],"metadata":{"id":"7bF5UAL6M373"}},{"cell_type":"markdown","source":["根据笔者先前的经验，不用想也知道，最后trainer要输出一个训练过的model，model肯定是要在trainer中被调用的。因此，依据自顶向下的拆解思路，我们选择先对trainer进行解析。"],"metadata":{"id":"78AEBX4gRd3J"}},{"cell_type":"code","source":["# 从全局来看，train_pretrain.py共有以下几个模块\n","def Logger(content):\n","  # 在非分布式训练或分布式训练回到0号机器时打印出content\n","  pass\n","\n","\n","def get_lr(current_step, total_step, lr):\n","  # 根据当前的步数和总步数更新学习率\n","  pass\n","\n","\n","def train_epoch(epoch, wandb):\n","  # 每轮训练的主干代码，给定训练轮次epoch和看板wandb，训练模型\n","  pass\n","\n","\n","def init_model(lm_config):\n","  # 根据给定的config初始化tokenizer和model，返回tokenizer和model\n","  pass\n","\n","\n","def init_distributed_mode():\n","  # 分布式训练的配置\n","  pass\n","\n","\n","def main():\n","  # 源代码的 \"if __name__ == \"__main__\":\" 部分。\n","  # 主函数，调用以上各函数开始训练循环\n","  pass"],"metadata":{"id":"4xFv3KW_KhMP"},"execution_count":null,"outputs":[]}]}