{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMdVMv4jbw6e2YjvZNNXesA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zMebSjyBAPkt"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!ls \"/content/drive/MyDrive/Colab Notebooks/LLM25/MinimindReproductionGuide/BreakdownProcess/dataset\"\n","import os\n","import sys\n","file_path = \"/content/drive/MyDrive/Colab Notebooks/LLM25/MinimindReproductionGuide/BreakdownProcess\"\n","os.chdir(file_path)"]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModel, AutoConfig\n","import os\n","\n","def convert_to_transformers(paths: list, model_name: str, save_path: str):\n","    \"\"\"\n","    将原生PyTorch模型权重转换为transformers库可加载的格式。\n","\n","    Args:\n","        paths (list): 原始PyTorch模型文件路径列表。\n","        model_name (str): 对应的transformers模型名称，用于加载正确的配置。\n","                          例如: 'bert-base-uncased', 'gpt2' 等。\n","        save_path (str): 转换后模型保存的本地目录路径。\n","    \"\"\"\n","    if not paths:\n","        print(\"模型路径列表为空，无法进行转换。\")\n","        return\n","\n","    # 1. 组合所有分片权重到一个字典\n","    full_state_dict = {}\n","    print(\"正在加载所有模型权重分片...\")\n","    for i, path in enumerate(paths):\n","        print(f\"  > 加载: {path}\")\n","        # 如果模型有多个分片，需要将它们合并\n","        shard_state_dict = torch.load(path, map_location='cpu')\n","        full_state_dict.update(shard_state_dict)\n","\n","    print(\"所有权重已加载。\")\n","\n","    # 2. 从 Hugging Face Hub 加载对应的模型配置\n","    # 这一步非常重要，因为 transformers 模型需要一个 config.json 文件\n","    # 来定义模型结构。\n","    try:\n","        config = AutoConfig.from_pretrained(model_name)\n","    except Exception as e:\n","        print(f\"加载模型 {model_name} 的配置失败，请检查模型名称是否正确。错误: {e}\")\n","        return\n","\n","    # 3. 创建一个新的 transformers 模型实例\n","    # 这里的 AutoModel 会根据 config 自动实例化正确的模型类。\n","    try:\n","        model = AutoModel.from_config(config)\n","    except Exception as e:\n","        print(f\"创建模型实例失败，请确保模型名称 {model_name} 与你的权重结构匹配。错误: {e}\")\n","        return\n","\n","    # 4. 匹配并加载权重\n","    print(\"正在将权重加载到新的 transformers 模型实例中...\")\n","    # 这里我们假设原始模型的 state_dict 键名与 transformers 库的命名方式完全一致。\n","    # 如果不一致，你需要在此处编写一个映射函数来处理键名转换。\n","\n","    # 例如，如果你的原始键名是 'fc1.weight'，而 transformers 是 'linear1.weight'，\n","    # 你需要像下面这样做：\n","    # mapped_state_dict = {\n","    #     'linear1.weight': full_state_dict['fc1.weight'],\n","    #     'linear1.bias': full_state_dict['fc1.bias'],\n","    #     ...\n","    # }\n","\n","    # 正常情况下，直接加载即可\n","    model.load_state_dict(full_state_dict)\n","\n","    print(\"权重加载成功。\")\n","\n","    # 5. 保存为 transformers 格式\n","    print(f\"正在将模型保存到 {save_path}...\")\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","    model.save_pretrained(save_path)\n","\n","    print(\"转换完成！你可以使用以下代码加载模型：\")\n","    print(f\"```python\\nfrom transformers import AutoModel\\nmodel = AutoModel.from_pretrained('{save_path}')\\n```\")\n","\n","# --- 使用示例 ---\n","if __name__ == '__main__':\n","    model_paths = [\"./out/pretrain_512.pth\", \"./out/full_sft_512.pth\", \"./out/lora_512.pth\", \"./out/rlhf_512.pth\"]\n","    hf_model_name = 'bert-base-uncased' # 确保这个名称和你模型的结构匹配\n","    output_directory = './miniformers'\n","    convert_to_transformers(model_paths, hf_model_name, output_directory)\n"],"metadata":{"id":"sQmykyMVAQUl"},"execution_count":null,"outputs":[]}]}