{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMtu8gQDrsyT/vtxDJHMKTL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## DataLoader\n","torch.utils.data.DataLoader 概述</br>\n","DataLoader 是 PyTorch 中一个核心的实用程序，用于迭代数据集。它将 torch.utils.data.Dataset 对象（代表您的数据）封装起来，提供了一种便捷且高效的方式来：\n","</br>\n","1. 批处理 (Batching)： 将单个样本组合成小批次，以充分利用 GPU 的并行计算能力。</br>\n","2. 乱序 (Shuffling)： 在每个 epoch 开始时打乱数据，以减少模型训练的偏差并提高泛化能力。</br>\n","3. 多进程数据加载 (Multi-process Data Loading / num_workers)： 使用多个子进程并行加载数据，加速数据预处理和传输，防止数据加载成为训练的瓶颈（I/O 瓶颈）。</br>\n","4. 内存固定 (Pin Memory)： 将加载的数据存储在 CUDA 可直接访问的内存中，加速数据从 CPU 到 GPU 的传输。</br>\n","\n","### 它常常包含以下几个组件：</br>\n","Sampler (索引生成器):DataLoader 不会直接调用 Dataset 的 __getitem__。</br>相反，它会使用一个 Sampler 来生成要获取的样本索引。</br>\n","SequentialSampler (默认): 按顺序生成 0 到 len(dataset)-1 的索引。</br>\n","RandomSampler (如果 shuffle=True): 在每个 epoch 开始时打乱索引并生成它们。</br>\n","BatchSampler (将索引聚合成批次): 将 Sampler 生成的单个索引聚合成批次的索引列表。DataLoader 默认会在内部使用 BatchSampler。</br>\n","DistributedSampler (分布式训练): 在分布式训练中，这个 Sampler 会确保每个 GPU 进程只处理数据集的一个不重叠的子集，并且可以进行乱序。</br>\n","\n","collate_fn (批处理函数):</br>\n","\n","当 DataLoader 从 Dataset 获取到单个样本的列表后（由 BatchSampler 提供的索引列表），它会调用 collate_fn 来将这些单个样本组合成一个批次的张量。</br>\n","默认的 collate_fn 会尝试将相同形状的张量堆叠（stack），并将 Python 数值类型转换为 PyTorch 张量。</br>\n","对于具有可变长度序列的数据（如 NLP 中的句子），您通常需要提供自定义的 collate_fn 来进行填充 (padding) 或其他特殊处理。</br>\n","num_workers (多进程加载):</br>\n","\n","如果 num_workers > 0，DataLoader 会创建指定数量的子进程。</br>\n","每个子进程独立地执行 Dataset 的 __getitem__ 方法，并将加载到的数据发送回主进程。</br>\n","这显著加快了数据加载速度，因为 CPU 上的数据预处理和 I/O 可以与 GPU 上的模型训练并行进行。</br>\n","pin_memory (内存固定):</br>\n","\n","如果 pin_memory=True 且在 CUDA 环境中，DataLoader 会将从 Dataset 获取到的数据存储在可由 CUDA 直接访问的 CPU 内存中（“pinned memory”或“page-locked memory”）。</br>\n","从 pinned memory 到 GPU 显存的数据传输比从常规 CPU 内存传输快得多。这减少了数据传输的延迟。</br>\n"],"metadata":{"id":"9UuCjJgxK9U-"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","# --- 1. 定义一个简单的Dataset ---\n","# 这个Dataset将生成 y = 2*x + 1 加上一些噪声的数据\n","class SimpleLinearDataset(Dataset):\n","    def __init__(self, num_samples=1000):\n","        self.num_samples = num_samples\n","        # 生成 x 值，形状为 [num_samples, 1]\n","        self.X = torch.randn(num_samples, 1) * 10 # 随机生成一些数据，范围[-10, 10]左右\n","        # 生成 y 值，基于线性关系 y = 2*x + 1，并添加噪声\n","        self.y = 2 * self.X + 1 + torch.randn(num_samples, 1) * 2 # 添加少量噪声\n","\n","    def __len__(self):\n","        \"\"\"返回数据集的样本总数\"\"\"\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        根据索引返回单个样本。\n","        DataLoader会多次调用这个方法来获取一个批次的数据。\n","        \"\"\"\n","        return self.X[idx], self.y[idx]"],"metadata":{"id":"vCYDs8UyTYcz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 2. 定义一个简单的模型 (线性回归) ---\n","class LinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super(LinearRegressionModel, self).__init__()\n","        # 输入特征是1维，输出也是1维\n","        self.linear = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"metadata":{"id":"DIWFNBaPTbvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3uQSXCeKyKd","executionInfo":{"status":"ok","timestamp":1748526649297,"user_tz":-480,"elapsed":9550,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"a355fe67-521a-4b52-ec5c-e344c241d60d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","Starting training...\n","Epoch [1/10], Average Loss: 246.2441\n","Epoch [2/10], Average Loss: 5.4720\n","Epoch [3/10], Average Loss: 6.8598\n","Epoch [4/10], Average Loss: 5.3417\n","Epoch [5/10], Average Loss: 4.2270\n","Epoch [6/10], Average Loss: 6.9698\n","Epoch [7/10], Average Loss: 4.0519\n","Epoch [8/10], Average Loss: 4.6585\n","Epoch [9/10], Average Loss: 4.0423\n","Epoch [10/10], Average Loss: 4.0123\n","Training finished.\n"]}],"source":["# --- 3. 设置训练参数 ---\n","num_samples = 1000 # 数据集总样本数\n","batch_size = 64    # 每个批次包含的样本数\n","num_epochs = 10    # 训练的轮数\n","learning_rate = 0.01\n","\n","# 检查是否有GPU可用\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# --- 4. 实例化Dataset和DataLoader ---\n","# 创建数据集实例\n","dataset = SimpleLinearDataset(num_samples=num_samples)\n","\n","# 创建DataLoader实例\n","# shuffle=True: 在每个epoch开始时打乱数据，这对训练很重要。\n","# num_workers=0: 在单机单线程情况下，通常设置为0。如果数据预处理复杂或数据量非常大，可以考虑设置为 >0 来利用多核CPU并行加载。\n","# pin_memory=True: 如果使用GPU，设置为True可以加速数据从CPU到GPU的传输。\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n","\n","# --- 5. 实例化模型、损失函数和优化器 ---\n","model = LinearRegressionModel().to(device) # 将模型移到指定设备\n","criterion = nn.MSELoss() # 均方误差损失，适用于回归任务\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate) # 使用随机梯度下降\n","\n","# --- 6. 训练循环 ---\n","print(\"\\nStarting training...\")\n","loss_history = [] # 记录损失变化\n","\n","for epoch in range(num_epochs):\n","    epoch_loss = 0.0\n","    # 迭代DataLoader，每次获取一个批次的数据\n","    for batch_idx, (inputs, targets) in enumerate(dataloader):\n","        # 将数据移到GPU (如果可用)\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # 前向传播\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # 反向传播和优化\n","        optimizer.zero_grad() # 清零梯度\n","        loss.backward()       # 计算梯度\n","        optimizer.step()      # 更新模型参数\n","\n","        epoch_loss += loss.item()\n","\n","    avg_epoch_loss = epoch_loss / len(dataloader)\n","    loss_history.append(avg_epoch_loss)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_epoch_loss:.4f}\")\n","\n","print(\"Training finished.\")"]}]}