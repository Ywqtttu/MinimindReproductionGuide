{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNSX57WUUNqxr45SHIX0U4r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## PretrainDataset</br>\n","PretrainDataset 不是 PyTorch 或任何标准库中预定义的函数或类。它是一个约定俗成的名称，通常指代专门为大型语言模型（LLMs）等模型的预训练阶段设计的数据集类。</br>\n","这类数据集的特点是需要处理海量的文本数据，并根据预训练任务（如 Masked Language Modeling, Next Sentence Prediction 等）对数据进行特殊处理。</br>\n","PretrainDataset 通常是 torch.utils.data.Dataset 的子类。其核心原理是实现 __len__ 和 __getitem__ 方法。</br>\n","### 运行原理：</br>\n","\n","懒加载 (Lazy Loading) / 流式处理： 大多数 PretrainDataset 不会一次性将所有数据读入内存。</br>相反，它们通常会：</br>\n","预处理： </br>将原始文本文件预处理成更易于加载的格式（如 jsonl 文件、二进制文件等），通常包含已标记化的 token IDs 和其他元数据。</br>\n","按需读取： </br>在 __getitem__ 方法被调用时，才从磁盘读取并处理单个或少量样本。</br>\n","任务定制化处理：</br> 在 __getitem__ 中，会根据当前预训练任务的需求对读取到的原始 token 序列进行进一步处理。</br>\n","### 例如：</br>\n","掩码语言模型 (MLM)： </br>随机选择一些 token 进行掩码（替换为 [MASK] token ID，或随机替换为其他 token，或保持不变），并记录被掩码的原始 token ID 作为目标。</br>\n","下一句预测 (NSP)： </br>从语料库中选取两个句子，判断它们是否是原文中的连续句子，并生成相应的标签和特殊的 [SEP] token。</br>\n","RoPE/位置编码： </br>虽然位置编码通常在模型 forward 中应用，但数据集可能会确保序列长度符合模型的要求，或者提供一些位置相关的元数据。</br>\n","缓存机制 (可选)： </br>为了提高效率，一些 PretrainDataset 可能会实现一个小的内存缓存，存储最近访问过的几个样本，避免频繁的磁盘 I/O。</br>\n","### 操作流程 (以常见的 MLM 任务为例)：\n","</br>\n","数据准备阶段 (离线/预处理)：\n","</br>\n","收集原始文本数据： </br>收集海量的文本语料（如维基百科、书籍、网页等）。</br>\n","分句/分段： </br>将文本分割成句子或段落。</br>\n","标记化 (Tokenization)： </br>\n","使用预训练模型对应的 Tokenizer（例如 BertTokenizer, GPT2Tokenizer 等）将文本转换为 token ID 序列。同时，添加特殊 token，如 [CLS], [SEP], [PAD]。</br>\n","序列化存储： </br>\n","将标记化后的 token ID 序列以及可能的元数据（如原始句子边界、文档边界）保存为高效的格式（如 TFRecord, HDF5, 或简单的 JSONL 文件）。</br>\n","这一步是关键，它避免了每次训练时都进行文本解析和标记化。</br>\n","\n","### PretrainDataset 初始化 (__init__)：</br>\n","\n","接收预处理后的数据文件路径、tokenizer、以及预训练任务相关的参数（如掩码概率 mlm_probability、最大序列长度 max_seq_length）。</br>\n","加载文件索引或元数据，但不加载实际数据到内存。</br>\n","__len__ 方法：</br>\n","\n","返回数据集中总样本的数量（例如，处理后的文档或片段的数量）。</br>\n","__getitem__(idx) 方法 (核心)：</br>\n","\n","读取原始数据：</br>\n","1. 根据 idx 从预处理文件中读取对应的 token ID 序列（可能是一个文档或两个相邻的段落）。</br>\n","2. 序列截断/填充： </br>\n","3. 根据 max_seq_length 对 token 序列进行截断或填充 ([PAD] token)。</br>\n","4. 应用预训练任务逻辑，比如以下几个常见的预训练任务逻辑要求：</br>\n","MLM 掩码： </br>\n","根据 mlm_probability 随机选择一些 token。对于这些 token，根据一定比例进行：</br>\n","替换为 [MASK] token ID。</br>\n","替换为随机 token ID。\n","保持不变。</br>\n","同时，生成一个 labels 序列，记录被掩码的原始 token ID，未被掩码的 token 处则用 -100（或其他忽略索引）标记。</br>\n","NSP 逻辑 ：</br>\n","随机选择一个真实下一句或一个随机非下一句。</br>\n","将它们拼接起来，用 [SEP] 分隔，并添加 [CLS] 在开头。</br>\n","生成一个 next_sentence_label (0 或 1)。</br>\n","生成 token_type_ids (Segment A 为 0，Segment B 为 1)。</br>\n","返回处理后的样本： 通常返回一个字典或元组，包含：</br>\n","input_ids: 经过掩码/拼接处理的 token ID 序列。</br>\n","attention_mask: 用于指示哪些 token 是真实内容，哪些是填充 ([PAD])。</br>\n","labels (MLM 目标): 记录被掩码的原始 token ID。</br>\n","token_type_ids (NSP 目标): 区分不同句子段。</br>\n","next_sentence_label (NSP 目标): 区分是否是下一句。</br>\n","torch.utils.data.DataLoader 包装：</br>\n","\n","将 PretrainDataset 实例传递给 DataLoader，以便进行批处理、乱序、多线程加载等操作。</br>"],"metadata":{"id":"u5ZY4quPE4Lc"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","import torch\n","from transformers import AutoTokenizer\n","import random"],"metadata":{"id":"Gn4FH8wJGomH","executionInfo":{"status":"ok","timestamp":1750929179379,"user_tz":-480,"elapsed":13417,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class MockTokenizer:\n","    def __init__(self):\n","        self.vocab = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[MASK]\": 3,\n","                      \"hello\": 4, \"world\": 5, \"this\": 6, \"is\": 7, \"a\": 8,\n","                      \"test\": 9, \"sentence\": 10, \"another\": 11, \"example\": 12,\n","                      \"data\": 13, \"for\": 14, \"pretraining\": 15, \"model\": 16}\n","        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n","        self.pad_token_id = self.vocab[\"[PAD]\"]\n","        self.cls_token_id = self.vocab[\"[CLS]\"]\n","        self.sep_token_id = self.vocab[\"[SEP]\"]\n","        self.mask_token_id = self.vocab[\"[MASK]\"]\n","\n","    def encode(self, text, add_special_tokens=False):\n","        # 简化版：直接将文本分割，并映射到ID\n","        tokens = text.lower().split()\n","        ids = [self.vocab.get(token, self.vocab[\"[MASK]\"]) for token in tokens] # 未知词也用MASK代替\n","        return ids\n","\n","    def convert_ids_to_tokens(self, ids):\n","        return [self.ids_to_tokens.get(id, \"[UNK]\") for id in ids]\n"],"metadata":{"id":"LzMx48jEIYlK","executionInfo":{"status":"ok","timestamp":1750929179385,"user_tz":-480,"elapsed":34,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["tokenizer = MockTokenizer()\n","# 模拟预处理后的文本数据 (这里直接用ID表示，方便演示)\n","# 真实场景中，这些会从文件读取\n","raw_preprocessed_data = [\n","    tokenizer.encode(\"Hello world this is a test sentence\"),\n","    tokenizer.encode(\"Another example for data pretraining\"),\n","    tokenizer.encode(\"This is another test sentence for model\"),\n","    tokenizer.encode(\"Hello world this is an example for pretraining\"),\n","    tokenizer.encode(\"A test data for a model\"),\n","    tokenizer.encode(\"Another sentence for the pretraining model\"),\n","    tokenizer.encode(\"Example data for the test model\"),\n","    tokenizer.encode(\"This is a world of pretraining\"),\n","]\n","\n","print(\"Simulated Raw Preprocessed Data (Token IDs):\")\n","for item in raw_preprocessed_data:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zN725gL1IYoF","executionInfo":{"status":"ok","timestamp":1750929179386,"user_tz":-480,"elapsed":33,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"4e1fb11f-5d21-4c49-93f1-1196e5747681"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Simulated Raw Preprocessed Data (Token IDs):\n","[4, 5, 6, 7, 8, 9, 10]\n","[11, 12, 14, 13, 15]\n","[6, 7, 11, 9, 10, 14, 16]\n","[4, 5, 6, 7, 3, 12, 14, 15]\n","[8, 9, 13, 14, 8, 16]\n","[11, 10, 14, 3, 15, 16]\n","[12, 13, 14, 3, 9, 16]\n","[6, 7, 8, 5, 3, 15]\n"]}]},{"cell_type":"code","source":["class SimplePretrainDataset(Dataset):\n","    def __init__(self, tokenized_data, tokenizer, max_seq_length=None, mlm_probability=0.15):\n","        self.tokenized_data = tokenized_data\n","        self.tokenizer = tokenizer\n","        self.max_seq_length = max_seq_length\n","        self.mlm_probability = mlm_probability\n","\n","        # 确保tokenizer有必要的特殊token ID\n","        assert hasattr(tokenizer, 'pad_token_id')\n","        assert hasattr(tokenizer, 'cls_token_id')\n","        assert hasattr(tokenizer, 'sep_token_id')\n","        assert hasattr(tokenizer, 'mask_token_id')\n","\n","    def __len__(self):\n","        return len(self.tokenized_data)\n","\n","    def __getitem__(self, idx):\n","        # 1. 获取原始 token IDs\n","        input_ids = self.tokenized_data[idx]\n","\n","        # 2. 添加特殊 token [CLS] 和 [SEP] (简化版，通常在预处理时完成，或在NSP任务中更复杂)\n","        # 比如： [CLS] + sentence_tokens + [SEP]\n","        input_ids = [self.tokenizer.cls_token_id] + input_ids + [self.tokenizer.sep_token_id]\n","\n","        # 3. 截断或填充\n","        if self.max_seq_length:\n","            if len(input_ids) > self.max_seq_length:\n","                input_ids = input_ids[:self.max_seq_length]\n","            else:\n","                # 填充 [PAD] token\n","                padding_length = self.max_seq_length - len(input_ids)\n","                input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n","\n","        # 4. 生成 attention_mask (1表示真实token，0表示填充token)\n","        attention_mask = [1] * len(input_ids)\n","        if self.max_seq_length and len(input_ids) < self.max_seq_length:\n","            attention_mask = attention_mask + [0] * (self.max_seq_length - len(attention_mask))\n","\n","        # 5. 执行 MLM (Masked Language Model) 掩码\n","        # labels 复制 input_ids，然后将未被掩码的位置设为 -100 (被PyTorch的CrossEntropyLoss忽略)\n","        labels = list(input_ids) # 复制一份作为标签\n","        masked_indices = self._mask_tokens(input_ids, labels) # 修改 input_ids 和 labels\n","\n","        return {\n","            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n","            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n","            \"labels\": torch.tensor(labels, dtype=torch.long)\n","        }\n","\n","    def _mask_tokens(self, inputs, labels):\n","        \"\"\"\n","        根据 mlm_probability 掩码 tokens，并更新 labels。\n","        这部分逻辑通常更复杂，以符合BERT等模型的掩码策略 (80% MASK, 10% 随机, 10% 不变)。\n","        这里是一个简化版本。\n","        \"\"\"\n","        masked_indices = []\n","        for i, token_id in enumerate(inputs):\n","            # 不掩码特殊 token\n","            if token_id in [self.tokenizer.cls_token_id, self.tokenizer.sep_token_id, self.tokenizer.pad_token_id]:\n","                continue\n","\n","            # 随机决定是否掩码\n","            if random.random() < self.mlm_probability:\n","                masked_indices.append(i)\n","                # 80% 几率替换为 [MASK]\n","                if random.random() < 0.8:\n","                    inputs[i] = self.tokenizer.mask_token_id\n","                # 10% 几率替换为随机 token\n","                elif random.random() < 0.5: # 0.8 + 0.1 = 0.9，所以这里是 <0.5 for the remaining 0.2\n","                    inputs[i] = random.randint(0, len(self.tokenizer.vocab) - 1)\n","                # 10% 几率保持不变 (labels仍然是原始token)\n","\n","                # 将 labels 中未被掩码的 token 设为 -100，表示在损失计算时忽略\n","            else:\n","                labels[i] = -100\n","        return masked_indices\n","\n","# 4. 使用 PretrainDataset 和 DataLoader\n","max_seq_len = 20 # 示例最大序列长度\n","mlm_prob = 0.15 # 掩码概率\n","\n","pretrain_dataset = SimplePretrainDataset(\n","    tokenized_data=raw_preprocessed_data,\n","    tokenizer=tokenizer,\n","    max_seq_length=max_seq_len,\n","    mlm_probability=mlm_prob\n",")\n","\n","pretrain_dataloader = DataLoader(pretrain_dataset, batch_size=2, shuffle=True)\n","\n","print(f\"\\nExample from DataLoader (batch_size=2, max_seq_len={max_seq_len}, mlm_probability={mlm_prob}):\")\n","# 获取一个批次数据并打印\n","for i, batch in enumerate(pretrain_dataloader):\n","    if i == 0: # 只打印第一个批次\n","        print(\"Input IDs (masked):\")\n","        print(batch[\"input_ids\"])\n","        print(\"\\nAttention Mask:\")\n","        print(batch[\"attention_mask\"])\n","        print(\"\\nLabels (original token IDs for masked positions, -100 otherwise):\")\n","        print(batch[\"labels\"])\n","\n","        # 转换为可读的tokens（仅用于演示）\n","        print(\"\\nInput Tokens (masked):\")\n","        for sample_ids in batch[\"input_ids\"]:\n","            print(tokenizer.convert_ids_to_tokens(sample_ids.tolist()))\n","        print(\"\\nLabel Tokens:\")\n","        for sample_labels in batch[\"labels\"]:\n","            # 过滤掉 -100\n","            readable_labels = [tokenizer.ids_to_tokens.get(id.item(), \"[UNK]\") if id.item() != -100 else -100 for id in sample_labels]\n","            print(readable_labels)\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdPdvZZYIYqG","executionInfo":{"status":"ok","timestamp":1750929179481,"user_tz":-480,"elapsed":119,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"cb2edf42-ec96-4bd0-a91d-7f956a6068de"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Example from DataLoader (batch_size=2, max_seq_len=20, mlm_probability=0.15):\n","Input IDs (masked):\n","tensor([[ 1, 12, 13, 14,  3,  9, 16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0],\n","        [ 1,  3, 12, 14, 13, 15,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0]])\n","\n","Attention Mask:\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","\n","Labels (original token IDs for masked positions, -100 otherwise):\n","tensor([[   1, -100, -100, -100,    3, -100, -100,    2,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0],\n","        [   1,   11, -100, -100, -100, -100,    2,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0]])\n","\n","Input Tokens (masked):\n","['[CLS]', 'example', 'data', 'for', '[MASK]', 'test', 'model', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","['[CLS]', '[MASK]', 'example', 'for', 'data', 'pretraining', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","\n","Label Tokens:\n","['[CLS]', -100, -100, -100, '[MASK]', -100, -100, '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","['[CLS]', 'another', -100, -100, -100, -100, '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"]}]}]}