{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNqUzBdlV0ylz79ubfohXi6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import random\n","\n","def generate_sample_data(num_samples=100, num_features=10, num_classes=3):\n","    \"\"\"\n","    生成用于分类任务的随机样例数据集。\n","\n","    Args:\n","        num_samples (int): 生成的样本数量。\n","        num_features (int): 每个样本的特征数量。\n","        num_classes (int): 类别数量。\n","\n","    Returns:\n","        tuple: (features_tensor, labels_tensor)\n","               features_tensor: torch.Tensor, 形状 (num_samples, num_features)\n","               labels_tensor: torch.Tensor, 形状 (num_samples,)\n","    \"\"\"\n","    # 随机生成特征数据 (X)\n","    features = torch.randn(num_samples, num_features)\n","\n","    # 随机生成标签 (y)，标签是 0 到 num_classes-1 之间的整数\n","    labels = torch.randint(0, num_classes, (num_samples,))\n","\n","    print(f\"生成的样本数量: {num_samples}\")\n","    print(f\"每个样本特征数: {num_features}\")\n","    print(f\"类别数量: {num_classes}\")\n","    print(f\"特征张量形状: {features.shape}\")\n","    print(f\"标签张量形状: {labels.shape}\")\n","    print(f\"部分特征示例:\\n{features[:3]}\")\n","    print(f\"部分标签示例:\\n{labels[:3]}\")\n","\n","    return features, labels\n","\n","if __name__ == '__main__':\n","    # 演示数据生成\n","    X_data, y_data = generate_sample_data(num_samples=20, num_features=5, num_classes=4)\n","\n","    # 模拟包含填充 token 的序列标签数据\n","    # 假设我们有一个批次，其中一些序列被填充，填充 token 的 ID 是 0\n","    # 在交叉熵损失中，我们希望忽略这些填充 token 的损失\n","    print(\"\\n--- 模拟带有 ignore_index 的序列标签数据 ---\")\n","    # 这是一个批次，包含 2 个序列，每个序列最大长度为 5\n","    # 类别有 3 个 (0, 1, 2)\n","    # 假设 0 是填充 token 的 ID (我们想忽略它)\n","    mock_logits_sequence = torch.randn(2, 5, 3) # batch_size=2, seq_len=5, num_classes=3\n","    mock_labels_sequence = torch.tensor([\n","        [0, 1, 2, 0, 0], # 第一个序列，标签 0, 1, 2，后面两个是填充\n","        [1, 0, 1, 2, 0]  # 第二个序列，标签 1, 0, 1, 2，最后一个是填充\n","    ])\n","    print(f\"模拟的序列 logits 形状: {mock_logits_sequence.shape}\")\n","    print(f\"模拟的序列 labels 形状: {mock_labels_sequence.shape}\")\n","    print(f\"模拟的序列 labels:\\n{mock_labels_sequence}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQl8vnahFhpZ","executionInfo":{"status":"ok","timestamp":1748573275196,"user_tz":-480,"elapsed":10602,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"47b8246a-ebc5-43be-958c-a2b4469a9c85"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["生成的样本数量: 20\n","每个样本特征数: 5\n","类别数量: 4\n","特征张量形状: torch.Size([20, 5])\n","标签张量形状: torch.Size([20])\n","部分特征示例:\n","tensor([[-0.8310,  0.1339,  0.6688, -0.1853, -0.6301],\n","        [ 0.9359, -0.4704, -1.3103,  0.4475, -0.8904],\n","        [-1.0550,  1.7600, -0.3360,  0.3945,  1.3438]])\n","部分标签示例:\n","tensor([0, 3, 2])\n","\n","--- 模拟带有 ignore_index 的序列标签数据 ---\n","模拟的序列 logits 形状: torch.Size([2, 5, 3])\n","模拟的序列 labels 形状: torch.Size([2, 5])\n","模拟的序列 labels:\n","tensor([[0, 1, 2, 0, 0],\n","        [1, 0, 1, 2, 0]])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# --- 1. 定义一个简单的分类模型 ---\n","class SimpleClassifier(nn.Module):\n","    def __init__(self, num_features, num_classes):\n","        super(SimpleClassifier, self).__init__()\n","        self.fc1 = nn.Linear(num_features, 64)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(64, num_classes)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        # 注意：这里不需要 sigmoid 或 softmax，CrossEntropyLoss 会自动处理\n","        x = self.fc2(x)\n","        return x\n","\n","# --- 2. 生成样例数据集 ---\n","num_samples = 1000\n","num_features = 20\n","num_classes = 5\n","X_data, y_data = generate_sample_data(num_samples, num_features, num_classes)\n","\n","# --- 3. 准备 DataLoader ---\n","dataset = TensorDataset(X_data, y_data)\n","batch_size = 32\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# --- 4. 实例化模型、损失函数和优化器 ---\n","model = SimpleClassifier(num_features, num_classes)\n","\n","# CrossEntropyLoss:\n","# - input (logits): (N, C)\n","# - target (labels): (N)\n","# 内部自动进行 LogSoftmax + NLLLoss\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# --- 5. 训练循环 ---\n","num_epochs = 10\n","print(\"\\n--- 开始训练 ---\")\n","for epoch in range(num_epochs):\n","    model.train() # 设置模型为训练模式\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for batch_features, batch_labels in dataloader:\n","        # 1. 前向传播\n","        outputs = model(batch_features) # outputs 是 logits (N, num_classes)\n","\n","        # 2. 计算损失\n","        # outputs: (batch_size, num_classes)\n","        # batch_labels: (batch_size)\n","        loss = loss_fn(outputs, batch_labels)\n","\n","        # 3. 反向传播和优化\n","        optimizer.zero_grad() # 清除之前的梯度\n","        loss.backward()       # 计算梯度\n","        optimizer.step()      # 更新模型参数\n","\n","        total_loss += loss.item()\n","\n","        # 计算准确率 (用于演示)\n","        _, predicted = torch.max(outputs.data, 1) # 获取预测的类别索引\n","        total_samples += batch_labels.size(0)\n","        correct_predictions += (predicted == batch_labels).sum().item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = 100 * correct_predictions / total_samples\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","\n","print(\"\\n--- 训练完成 ---\")\n","\n","# --- 6. 演示带有 `ignore_index` 的 CrossEntropyLoss ---\n","print(\"\\n--- 演示带有 `ignore_index` 的 CrossEntropyLoss ---\")\n","# 模拟序列任务的 logits 和 labels\n","# 假设有 2 个序列，每个序列长度为 5，3 个类别 (0, 1, 2)\n","# 假设 0 是填充 token 的 ID，我们希望忽略它的损失贡献\n","mock_logits_sequence = torch.randn(2, 5, 3) # batch_size=2, seq_len=5, num_classes=3\n","mock_labels_sequence = torch.tensor([\n","    [0, 1, 2, 0, 0], # 第一个序列，标签 0, 1, 2，后面两个 0 是填充\n","    [1, 0, 1, 2, 0]  # 第二个序列，标签 1, 0, 1, 2，最后一个 0 是填充\n","])\n","\n","# 重新形状 logits 以适应 CrossEntropyLoss 的输入要求\n","# (N, C) N=batch_size * seq_len\n","# 将 mock_logits_sequence 形状从 (2, 5, 3) 变为 (2*5, 3) = (10, 3)\n","reshaped_logits = mock_logits_sequence.view(-1, num_classes)\n","# 将 mock_labels_sequence 形状从 (2, 5) 变为 (2*5) = (10,)\n","reshaped_labels = mock_labels_sequence.view(-1)\n","\n","print(f\"重塑后的 logits 形状: {reshaped_logits.shape}\")\n","print(f\"重塑后的 labels 形状: {reshaped_labels.shape}\")\n","print(f\"重塑后的 labels (含填充): {reshaped_labels}\")\n","\n","# 实例化带有 ignore_index 的损失函数\n","# 假设我们的填充 token 的 ID 是 0\n","loss_fn_ignore = nn.CrossEntropyLoss(ignore_index=0) # 忽略标签为 0 的损失\n","\n","loss_with_ignore = loss_fn_ignore(reshaped_logits, reshaped_labels)\n","print(f\"使用 ignore_index=0 计算的损失: {loss_with_ignore.item():.4f}\")\n","\n","# 比较：如果不忽略填充\n","loss_without_ignore = nn.CrossEntropyLoss()(reshaped_logits, reshaped_labels)\n","print(f\"不使用 ignore_index 计算的损失: {loss_without_ignore.item():.4f}\")\n","print(\"可以看出，当指定 ignore_index 后，损失值会发生变化，因为它忽略了特定标签的贡献。\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":998},"id":"aaorCRkmFhrQ","executionInfo":{"status":"error","timestamp":1748573303149,"user_tz":-480,"elapsed":7687,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"d21a8087-688c-4670-edf7-debad7d3ec9e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["生成的样本数量: 1000\n","每个样本特征数: 20\n","类别数量: 5\n","特征张量形状: torch.Size([1000, 20])\n","标签张量形状: torch.Size([1000])\n","部分特征示例:\n","tensor([[ 0.3187,  0.1825,  0.7392, -2.5795, -0.7172,  0.8008,  1.6403,  1.4680,\n","          1.6394, -0.1548, -0.4266,  0.5731, -1.1909, -1.0562,  0.1423, -0.3859,\n","          1.4876,  0.9411,  0.3705,  1.4085],\n","        [ 0.4400,  0.2762,  0.0961, -1.0619, -1.6829,  1.0217, -1.4161, -0.8497,\n","         -1.1269,  1.8879,  0.2182, -0.6327,  2.1705,  0.3682, -1.2846, -0.6902,\n","         -0.1661,  1.5184, -0.1362, -0.4505],\n","        [-0.2543, -1.0076, -0.3582,  0.0697, -0.2602,  0.4723,  2.1217, -0.5414,\n","         -1.1001, -1.0351,  1.2462, -2.0808, -0.0427,  0.3726, -0.3560,  1.2486,\n","         -0.6592,  0.6755,  0.0840,  0.9176]])\n","部分标签示例:\n","tensor([3, 1, 1])\n","\n","--- 开始训练 ---\n","Epoch [1/10], Loss: 1.6529, Accuracy: 17.10%\n","Epoch [2/10], Loss: 1.5700, Accuracy: 27.90%\n","Epoch [3/10], Loss: 1.5480, Accuracy: 29.60%\n","Epoch [4/10], Loss: 1.5036, Accuracy: 33.20%\n","Epoch [5/10], Loss: 1.4602, Accuracy: 36.00%\n","Epoch [6/10], Loss: 1.4421, Accuracy: 39.50%\n","Epoch [7/10], Loss: 1.3819, Accuracy: 42.00%\n","Epoch [8/10], Loss: 1.3388, Accuracy: 45.60%\n","Epoch [9/10], Loss: 1.2833, Accuracy: 49.40%\n","Epoch [10/10], Loss: 1.2810, Accuracy: 47.10%\n","\n","--- 训练完成 ---\n","\n","--- 演示带有 `ignore_index` 的 CrossEntropyLoss ---\n","重塑后的 logits 形状: torch.Size([6, 5])\n","重塑后的 labels 形状: torch.Size([10])\n","重塑后的 labels (含填充): tensor([0, 1, 2, 0, 0, 1, 0, 1, 2, 0])\n"]},{"output_type":"error","ename":"ValueError","evalue":"Expected input batch_size (6) to match target batch_size (10).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2a544232fd4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mloss_fn_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 忽略标签为 0 的损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mloss_with_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_ignore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"使用 ignore_index=0 计算的损失: {loss_with_ignore.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (6) to match target batch_size (10)."]}]},{"cell_type":"code","source":[],"metadata":{"id":"UOiIrym0Fhth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X-76z0n3Fhvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fGsk0RsXFhxi"},"execution_count":null,"outputs":[]}]}