{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOkM9ia4yYe0rRe7gaLCYjK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!ls \"/content/drive/MyDrive/Colab Notebooks/LLM25/MinimindReproductionGuide/BreakdownProcess/dataset\"\n","import os\n","import sys\n","file_path = \"/content/drive/MyDrive/Colab Notebooks/LLM25/MinimindReproductionGuide/BreakdownProcess\"\n","os.chdir(file_path)\n","__package__ == \"trainer\""],"metadata":{"id":"WeUWkoI9LiR_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756461924761,"user_tz":-480,"elapsed":22385,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"69412a3a-a91d-42b3-8f97-995abec6e4d7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","dpo_test.jsonl\t __init__.py\t   lm_dataset.py      __pycache__\n","dpo_train.jsonl  lm_dataset.ipynb  pretrain_hq.jsonl  sft_mini_512.jsonl\n"]},{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["'''\n","from datasets import load_dataset\n","\n","# 1. 加载数据集\n","ds = load_dataset(\"Magpie-Align/MagpieLM-DPO-Data-v0.1\")\n","\n","# 2. 选择一个拆分\n","train_ds = ds['train']\n","\n","# 3. 将数据集转换为 JSONL 文件并保存\n","train_ds.to_json('magpie_dpo_train.jsonl', lines=True)\n","\n","\n","test_ds = ds['test']\n","test_ds.to_json('magpie_dpo_test.jsonl', lines=True)\n","\n","print(\"数据集已成功转换为 JSONL 文件并保存到本地。\")\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"Oo49zM-5Z6KZ","executionInfo":{"status":"ok","timestamp":1756461924781,"user_tz":-480,"elapsed":10,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"59253d30-5903-455b-dcb4-04f46f54ded2"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom datasets import load_dataset\\n\\n# 1. 加载数据集\\nds = load_dataset(\"Magpie-Align/MagpieLM-DPO-Data-v0.1\")\\n\\n# 2. 选择一个拆分\\ntrain_ds = ds[\\'train\\']\\n\\n# 3. 将数据集转换为 JSONL 文件并保存\\ntrain_ds.to_json(\\'magpie_dpo_train.jsonl\\', lines=True)\\n\\n\\ntest_ds = ds[\\'test\\']\\ntest_ds.to_json(\\'magpie_dpo_test.jsonl\\', lines=True)\\n\\nprint(\"数据集已成功转换为 JSONL 文件并保存到本地。\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import time\n","import math\n","import warnings\n","import torch\n","import torch.nn.functional as F\n","from contextlib import nullcontext\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n","from dataset.lm_dataset import DPODataset\n","from datasets import load_dataset\n","warnings.filterwarnings('ignore')\n","\n","\n","def Logger(content):\n","    print(content)\n","\n","\n","def get_lr(current_step, total_steps, lr):\n","    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n","\n","\n","def logits_to_probs(logits, labels):\n","    log_probs = F.log_softmax(logits, dim=2)\n","    probs = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(-1)\n","    return probs\n","\n","\n","def init_model(lm_config):\n","    tokenizer = AutoTokenizer.from_pretrained('./model/')\n","    model = MiniMindForCausalLM(lm_config)\n","    # sft_path = \"./out/full_sft_512.pth\"\n","    sft_path = \"./out/rlhf_512.pth\"\n","    state_dict = torch.load(sft_path, map_location=args.device)\n","    model.load_state_dict(state_dict, strict=False)\n","    # 初始化参考模型\n","    ref_model = MiniMindForCausalLM(lm_config)\n","    ref_model.load_state_dict(state_dict, strict=False)\n","    ref_model.eval()\n","    ref_model.requires_grad_(False)\n","\n","    Logger(f'LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n","    model = model.to(args.device)\n","    ref_model = ref_model.to(args.device)\n","\n","    return model, ref_model, tokenizer\n"],"metadata":{"id":"8d1Iae-oLraY","executionInfo":{"status":"ok","timestamp":1756461966599,"user_tz":-480,"elapsed":41813,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def dpo_loss(ref_probs, probs, mask, beta):\n","    # ref_probs 和 probs 都是 shape: (batch_size, seq_len)\n","    seq_lengths = mask.sum(dim=1, keepdim=True)  # (batch_size, 1)\n","    ref_probs = (ref_probs * mask).sum(dim=1) / seq_lengths.squeeze()\n","    probs = (probs * mask).sum(dim=1) / seq_lengths.squeeze()\n","\n","    # 将 chosen 和 rejected 数据分开\n","    batch_size = ref_probs.shape[0]\n","    chosen_ref_probs = ref_probs[:batch_size // 2]\n","    reject_ref_probs = ref_probs[batch_size // 2:]\n","    chosen_probs = probs[:batch_size // 2]\n","    reject_probs = probs[batch_size // 2:]\n","\n","    pi_logratios = chosen_probs - reject_probs\n","    ref_logratios = chosen_ref_probs - reject_ref_probs\n","    logits = pi_logratios - ref_logratios\n","    loss = -F.logsigmoid(beta * logits)\n","    return loss.mean()"],"metadata":{"id":"FdAKyABaL2Rz","executionInfo":{"status":"ok","timestamp":1756461966611,"user_tz":-480,"elapsed":15,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Arguments:\n","  def __init__(self,\n","      out_dir = \"./out\",\n","      epochs = 1,\n","      batch_size = 32,\n","      learning_rate = 1e-8,\n","      device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","      dtype = \"bfloat16\",\n","      accumulation_steps = 8,\n","      grad_clip = 1,\n","      warmup_iters = 0,\n","      log_interval = 100,\n","      save_interval = 100,\n","      local_rank = -1,\n","      hidden_size = 512,\n","      num_hidden_layers = 8,\n","      max_seq_len = 512,\n","      data_path = \"./dataset/dpo_train.jsonl\"\n","  ):\n","    self.out_dir = out_dir\n","    self.epochs = epochs\n","    self.batch_size = batch_size\n","    self.learning_rate = learning_rate\n","    self.device = device\n","    self.dtype = dtype\n","    self.accumulation_steps = accumulation_steps\n","    self.grad_clip = grad_clip\n","    self.warmup_iters = warmup_iters\n","    self.log_interval = log_interval\n","    self.save_interval = save_interval\n","    self.local_rank = local_rank\n","    self.hidden_size = hidden_size\n","    self.num_hidden_layers = num_hidden_layers\n","    self.max_seq_len = max_seq_len\n","    self.data_path = data_path\n","    self.save_dir = None\n","    self.tokens_per_iter = self.batch_size * self.max_seq_len"],"metadata":{"id":"kY_U3dPLOHDR","executionInfo":{"status":"ok","timestamp":1756461966612,"user_tz":-480,"elapsed":8,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"FK58hl-kLOSn","executionInfo":{"status":"ok","timestamp":1756461966627,"user_tz":-480,"elapsed":16,"user":{"displayName":"易文乾","userId":"13136328038158270412"}}},"outputs":[],"source":["def train_epoch(epoch):\n","    start_time = time.time()\n","    for step, batch in enumerate(train_loader):\n","        x_chosen = batch['x_chosen'].to(args.device)\n","        x_rejected = batch['x_rejected'].to(args.device)\n","        y_chosen = batch['y_chosen'].to(args.device)\n","        y_rejected = batch['y_rejected'].to(args.device)\n","        mask_chosen = batch['mask_chosen'].to(args.device)\n","        mask_rejected = batch['mask_rejected'].to(args.device)\n","        x = torch.cat([x_chosen, x_rejected], dim=0)\n","        y = torch.cat([y_chosen, y_rejected], dim=0)\n","        mask = torch.cat([mask_chosen, mask_rejected], dim=0)\n","\n","        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","        with ctx:\n","            with torch.no_grad():\n","                ref_outputs = ref_model(x)\n","                ref_logits = ref_outputs.logits\n","            ref_probs = logits_to_probs(ref_logits, y)\n","            ref_probs = ref_probs * mask\n","            outputs = model(x)\n","            logits = outputs.logits\n","            probs = logits_to_probs(logits, y)\n","            probs = probs * mask\n","            loss = dpo_loss(ref_probs, probs, mask, beta=0.1)\n","            loss = loss / args.accumulation_steps\n","\n","        scaler.scale(loss).backward()\n","\n","        if (step + 1) % args.accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad(set_to_none=True)\n","\n","        if step % args.log_interval == 0:\n","            spend_time = time.time() - start_time\n","            Logger(\n","                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n","                    epoch + 1,\n","                    args.epochs,\n","                    step,\n","                    iter_per_epoch,\n","                    loss.item() * args.accumulation_steps,\n","                    optimizer.param_groups[-1]['lr'],\n","                    spend_time))\n","\n","        if (step + 1) % args.save_interval == 0:\n","            model.eval()\n","            ckp = f'./out/rlhf_{lm_config.hidden_size}.pth'\n","\n","            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n","                state_dict = model.module.state_dict()\n","            else:\n","                state_dict = model.state_dict()\n","            state_dict = {k: v.half() for k, v in state_dict.items()}  # 半精度保存\n","            torch.save(state_dict, ckp)\n","            model.train()"]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    args = Arguments()\n","\n","    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers)\n","    tokens_per_iter = args.batch_size * args.max_seq_len\n","    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n","    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n","    base_seed = 114514\n","    torch.manual_seed(base_seed)\n","    torch.cuda.manual_seed(base_seed)\n","\n","    model, ref_model, tokenizer = init_model(lm_config)\n","\n","    train_ds = DPODataset(file_path = args.data_path, tokenizer = tokenizer, max_length=args.max_seq_len)\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=args.batch_size,\n","        pin_memory=True,\n","        drop_last=False,\n","        shuffle=False\n","    )\n","\n","    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n","    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n","\n","    iter_per_epoch = len(train_loader)\n","    for epoch in range(args.epochs):\n","        train_epoch(epoch)"],"metadata":{"id":"X0y8LCIyMRzr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a45a5ebc-1c8f-4b26-914f-38538f789945"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM总参数量：25.830 百万\n","Epoch:[1/2000](0/4657) loss:nan lr:0.000000011000 epoch_Time:2.020221471786499min:\n","Epoch:[1/2000](100/4657) loss:nan lr:0.000000011000 epoch_Time:88.77718877792358min:\n","Epoch:[1/2000](200/4657) loss:nan lr:0.000000011000 epoch_Time:169.57017850875854min:\n","Epoch:[1/2000](300/4657) loss:nan lr:0.000000011000 epoch_Time:251.02667713165283min:\n","Epoch:[1/2000](400/4657) loss:nan lr:0.000000011000 epoch_Time:333.02969765663147min:\n","Epoch:[1/2000](500/4657) loss:nan lr:0.000000011000 epoch_Time:413.824444770813min:\n","Epoch:[1/2000](600/4657) loss:nan lr:0.000000011000 epoch_Time:494.73823714256287min:\n","Epoch:[1/2000](700/4657) loss:nan lr:0.000000011000 epoch_Time:575.4758398532867min:\n","Epoch:[1/2000](800/4657) loss:nan lr:0.000000011000 epoch_Time:655.8162634372711min:\n","Epoch:[1/2000](900/4657) loss:nan lr:0.000000011000 epoch_Time:736.0990598201752min:\n"]}]}]}