{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMV1gbXd9/64Iqv9DyLC605"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## .view()\n","一个重塑张量形状的方法，相当于numpy的reshape,但是view的方便之处在于，可以采用-1占位符，比如我希望将[10, 10]的矩阵转换为[2, 50]，我就可以使用x.view(-1, 50)。"],"metadata":{"id":"jv-fkkUEPWTR"}},{"cell_type":"code","source":["import torch\n","\n","# 原始张量\n","a = torch.arange(12) # a: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","print(f\"原始张量 a: {a}, 形状: {a.shape}\") # 形状: torch.Size([12])\n","\n","# 改变为 (3, 4) 的形状\n","b = a.view(3, 4)\n","print(f\"b: {b}, 形状: {b.shape}\")\n","# b:\n","# tensor([[ 0,  1,  2,  3],\n","#         [ 4,  5,  6,  7],\n","#         [ 8,  9, 10, 11]]), 形状: torch.Size([3, 4])\n","\n","# 改变为 (2, 2, 3) 的形状\n","c = a.view(2, 2, 3)\n","print(f\"c: {c}, 形状: {c.shape}\")\n","# c:\n","# tensor([[[ 0,  1,  2],\n","#          [ 3,  4,  5]],\n","#\n","#         [[ 6,  7,  8],\n","#          [ 9, 10, 11]]]), 形状: torch.Size([2, 2, 3])\n","\n","# 错误示例：总元素数量不匹配\n","try:\n","    d = a.view(3, 5) # 总元素数量为 15，与原始 12 不匹配\n","except RuntimeError as e:\n","    print(f\"错误: {e}\")\n","# 错误: shape '[3, 5]' is invalid for input of size 12\n","\n","# 链式操作：如果原始张量不是连续的\n","# d = torch.randn(2, 3, 4).permute(0, 2, 1) # permute 操作可能导致非连续\n","# print(f\"d 是否连续: {d.is_contiguous()}\") # False\n","# e = d.contiguous().view(-1, 3) # 需要先 contiguous()"],"metadata":{"id":"oUlAGT-fJVJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LossMask\n","顾名思义，损失掩码就是对Loss的结果进行覆盖和修改，使得无效值的影响能够被排除，模型参数能够得到更准确更精细的优化</br>\n","需要注意，loss_mask不会直接在result或者targets上操作，因为这样会影响二者本身的数据特征，通常是在计算出loss之后，基于原来的输入，创建一个和input_ids同形状的0-1矩阵，将无效值（通常是填充值或者tokenizer的特殊标记位置）的loss打为0，其余不变，从而精细化Loss。</br>\n"],"metadata":{"id":"3EHqI19NN1Nc"}}]}