{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMgEt5JwQ3aLWnes9m39edH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## torch.cuda.amp.autocast\n","这是一个实现**自动混合精度训练（Automatic Mixed Precision, AMP）**的关键上下文管理器</br>\n"," AMP 的思想是，在训练过程中，根据操作的类型和敏感度，混合使用 FP16 和 FP32 两种精度以节约显存</br>\n"," 操作类型感知： autocast 在其上下文管理器中执行代码时，会检查每个 PyTorch 操作（例如，矩阵乘法 torch.matmul，卷积 nn.Conv2d，激活函数 nn.ReLU 等）。</br>\n","\n","动态类型提升 (Dynamic Type Promotion)：</br>\n","\n","对于那些在 FP16 下效率更高且数值稳定性较好的操作（例如，矩阵乘法、卷积），autocast 会自动将输入张量转换为 FP16，并使用 FP16 执行这些操作。</br>\n","对于那些在 FP16 下可能存在数值不稳定（如 Softmax、LogSoftmax、BN 层的统计计算等）或精度要求较高的操作，autocast 会自动将输入提升回 FP32 执行这些操作，以避免精度损失。</br>\n","这种判断和转换是在运行时动态发生的，由 PyTorch 内部的逻辑管理。</br>\n","梯度计算：</br>\n","前向传播（forward pass）中的计算结果（激活值）可能存储为 FP16。</br>\n","在反向传播（backward pass）时，梯度也需要计算。为了确保数值稳定性，PyTorch AMP 通常会使用一个叫做 torch.cuda.amp.GradScaler 的组件。</br>\n","## GradScaler 的作用是进行损失缩放（Loss Scaling）。</br>\n","由于 FP16 的数值范围较小，如果梯度值太小，它们可能会下溢为零，导致参数无法更新。</br>GradScaler 会在计算损失后，将其乘以一个大的缩放因子，使得梯度值变大，避免下溢。在反向传播计算完成后，梯度在传回 FP32 参数之前，会被反向缩放，恢复其真实大小。"],"metadata":{"id":"i_1XTqbu78Kz"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import Dataset, DataLoader\n","\n","# 1. 定义一个简单的Dataset\n","class SimpleDataset(Dataset):\n","    def __init__(self, num_samples=1000, input_dim=10, output_dim=1):\n","        self.num_samples = num_samples\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        # 生成随机输入数据和对应的线性目标\n","        self.X = torch.randn(num_samples, input_dim)\n","        self.y = torch.randn(num_samples, output_dim) * 5 + 10 # 简单模拟一些变化\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]"],"metadata":{"id":"smSG1nV57j6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. 自定义一个简单的演示模型\n","class MyModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer1 = nn.Linear(10, 20)\n","    self.relu = nn.ReLU()\n","    self.layer2 = nn.Linear(20, 1)\n","\n","  def forward(self, x):\n","    x = self.layer1(x)\n","    x = self.relu(x)\n","    x = self.layer2(x)\n","    return x"],"metadata":{"id":"aHz9X8K77-9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MyModel().cuda()\n","# 2. 实例化一个优化器\n","optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n","# 3. 实例化一个损失函数\n","criterion = nn.MSELoss()\n","# 4. 实例化一个GradScaler，进行梯度缩放，预防因为FP16导致梯度消失\n","scaler = GradScaler()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkwolvDo7-_d","executionInfo":{"status":"ok","timestamp":1750929080766,"user_tz":-480,"elapsed":4896,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"6ee27aef-44f8-4f70-c6d5-a718736b2d62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3-2770131166.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n"]}]},{"cell_type":"code","source":["def main():\n","  for epoch in range(11451):\n","    for batch_idx, (data, target) in enumerate(dataloader):\n","      data, target = data.cuda(), target.cuda()\n","      optim.zero_grad()\n","      # 梯度归零\n","\n","      # 5. 使用上下文管理器autocast\n","      with autocast():\n","        output = model(data)\n","        loss = criterion(output, target)\n","      # 6. 使用scaler进行梯度缩放\n","      scaler.scale(loss).backward()\n","      # 它会检查梯度是否有效（非Inf和Nan），然后缩放梯度并更新参数\n","      scaler.step(optim)\n","\n","      # 7. 缩放因子，假如在上面没有无效梯度，则缩放因子会稍微增加\n","      scaler.update()\n","\n","      if batch_idx % 100 == 0:\n","        print(f\"Epoch{epoch}, Batch{batch_idx}, Loss:{loss.item():.4f}\")\n"],"metadata":{"id":"lGGN8pKu7_Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = 10\n","output_dim = 1\n","num_samples = 10000 # 更多数据\n","batch_size = 64\n","num_epochs = 10 # 增加训练周期\n","\n","dataset = SimpleDataset(num_samples=num_samples, input_dim=input_dim, output_dim=output_dim)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"SURy4479CeeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"CXXNmvRSKohm"},"execution_count":null,"outputs":[]}]}