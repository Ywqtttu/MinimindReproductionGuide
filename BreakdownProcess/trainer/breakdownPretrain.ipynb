{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPkSyRooTW+Rb9LuydOH/hH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 接下来自顶向下，也就是从main方法开始，逐个分析各函数的算法流程和依赖关系。\n","## 1. train_pretrain.py\n","\n","### 1.1 main()\n","\n","#### 1.1.1 处理流程\n","\n","- parser定义各种超参数\n","\n","- args存储parser中的超参数\n","\n","- 实例化MiniMindConfig为lm_config\n","- 设置存储路径等\n","- 设置token字典的长度\n","- 初始化ctx\n","- 配置分布式训练\n","- 初始化wandb\n","- 初始化model和tokenizer\n","- 初始化PretrainDataset: train_ds\n","- 初始化分布式样例\n","- 初始化DataLoader: train_loader\n","- 初始化scaler\n","- 初始化optimizer\n","- 调用train_epoch, 在for循环中开始训练\n","\n","#### 1.1.2 依赖函数\n","\n","| Name                          | Description                            |\n","| ----------------------------- | -------------------------------------- |\n","| **argparse**                  | 初始化一个parser，设置并存储各种超参数 |\n","| **MiniMindConfig**            | Minimind模型设置                       |\n","| **ctx**                       | 一个能够提高计算效率的计算器           |\n","| **wandb**                     | 一个数据看板和模型优化器               |\n","| **init_model**                | 初始化model和tokenizer                 |\n","| **PretrainDataset**           | 实例化预训练数据集                     |\n","| **DistributedSampler**        | 分布式训练相关                         |\n","| **DataLoader**                | 生成数据迭代器                         |\n","| **torch.cuda.amp.GradScaler** | 一个控制模型参数提高优化效率的计算器   |\n","| **optim**                     | 优化器                                 |\n","| **train_epoch**               | 单轮训练方法                           |\n","\n","### 1.2 argparse\n","\n","- 使用argparse.ArgumentParser实例化一个parser，\n","\n","然后使用\n","\n","- add_argument(name_or_flags = , type = , default = )\n","\n","可以逐个往解析器中添加参数。\n","\n","- parser.parse_args() 可以返回所有参数，以类似字典的形式。\n","\n","**无显式的依赖函数**\n","\n","### 1.3 MiniMindConfig\n","\n","继承了PretrainConfig类，用于设置模型的各种参数。\n","\n","**无显式的依赖函数**\n","\n","### 1.4 ctx\n","\n","一个更高效的计算器，使用它计算会极大提高效率\n","\n","**无显式的依赖函数**\n","\n","### 1.5 wandb\n","\n","算是一个数据看板+模型调优的小工具\n","\n","**无显式的依赖函数**\n","\n","### 1.6 init_model\n","\n","初始化tokenizer和model，并且打印Log\n","\n","| Name                | Description        |\n","| ------------------- | ------------------ |\n","| AutoTokenizer       | 实例化tokenizer    |\n","| MiniMindForCausalLM | 实例化MiniMind模型 |\n","\n","### 1.7 AutoTokenizer\n","\n","读取json文件，实例化一个tokenizer\n","\n","**无显式依赖函数**\n","\n","### 1.8 MiniMindForCausalLM\n","\n","**实例化一个主模型，具体结构再议**\n","\n","### 1.9 PretrainDataset\n","\n","继承了torch的Dataset类，读取指定路径的文件，实例化数据集。\n","\n","**无显式的依赖**\n","\n","### 1.10 DistributedSampler\n","\n","略。\n","\n","### 1.11 DataLoader\n","\n","接收一个Dataset的子类实例和其他相关参数，生成数据迭代器（其实也可以用手动for循环代替）\n","\n","**无显式的依赖**\n","\n","### 1.12 torch.cuda.amp.GradScaler\n","\n","动态管理梯度缩放的一个扫描器，在参数优化的时候把optimizer放在这里边跑，好处多多。\n","\n","**无显式的依赖**\n","\n","### 1.13 optim\n","\n","优化器，老生常谈\n","\n","**无显式的依赖**\n","\n","### 1.14 train_epoch\n","\n","接收一个轮次，一个看板实例wandb\n","\n","- 实例化交叉熵计算器\n","- 记录开始计算的时间\n","- 载入数据迭代器，for每一个step\n","  - 将数据放到GPU上\n","  - 更新学习率\n","  - for optimizer里的每一个param\n","    - 更新学习率\n","  - 使用ctx计算\n","    - 模型当前的残差\n","    - 模型当前的loss\n","  - 使用scaler后向传播优化参数\n","  - 假如经过了特定步数的训练，要进行梯度更新控制\n","    - 取消缩放\n","    - 裁剪梯度\n","    - 参数更新\n","    - 更新缩放器\n","    - 清空梯度\n","  - 假如经过了特定步数的训练，记录训练日志\n","    - 修正损失值\n","    - 获取学习率\n","    - 估算计算时间\n","    - 同步日志到wandb\n","  - 对于专家模型有另外安排，此处略。\n","\n","| Name                 | Description  |\n","| -------------------- | ------------ |\n","| **CrossEntropyLoss** | 交叉熵计算器 |\n","| **get_lr**           | 更新学习率   |\n","| **scaler**           | 梯度缩放器   |\n","| **optimizer**        | 优化器       |\n","| **Logger**           | 记录日志     |</br>\n","绘制一个不标准的UML图如下："],"metadata":{"id":"oHaqXPwh4-yI"}},{"cell_type":"markdown","source":["# 根据以上的拆解，我们开始尝试自底向上复现train_pretrain.py的各个模块。"],"metadata":{"id":"2RMBLbya58X2"}},{"cell_type":"code","source":["import os\n","import sys\n","__package__ = \"trainer\"\n","# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n","\n","import time\n","import math\n","import warnings\n","import torch\n","from torch import optim, nn\n","from torch.utils.data import DataLoader\n","from contextlib import nullcontext\n","from transformers import AutoTokenizer\n","# from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n","# from dataset.lm_dataset import PretrainDataset\n","\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"L_DnpR1EIadN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Logger(content):\n","  # 由于笔者暂时不考虑分布式训练，因此此处以及以下各处与源码有所不同。\n","  print(content)"],"metadata":{"id":"Qqvhy_JZMe9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_lr(current_step, total_step, lr):\n","  # 余弦退火调度学习率，随着训练的进行，学习率逐渐减小但不至于为0\n","  return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))"],"metadata":{"id":"fqckC-dl5Ijc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_model(lm_config):\n","    # 自动读取目标目录的json文件，初始化为tokenizer对象\n","    tokenizer = AutoTokenizer.from_pretrained('../model/')\n","    model = MiniMindForCausalLM(lm_config).to(args.device)\n","    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n","    return model, tokenizer"],"metadata":{"id":"YAB8xYiiMfxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Arguments:\n","  def __init__(self,\n","      out_dir = \"../out\",\n","      epochs = '1',\n","      batch_size = '32',\n","      learning_rate = '5e-4',\n","      device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","      dtype = \"bfloat16\",\n","      accumulation_steps = 8,\n","      grad_clip = 1,\n","      warmup_iters = 0,\n","      log_interval = 100,\n","      save_interval = 100,\n","      local_rank = -1,\n","      hidden_size = 512,\n","      num_hidden_layers = 8,\n","      max_seq_len = 512,\n","      data_path = \"../dataset/pretrain_hq.jsonl\"\n","  ):\n","    self.out_dir = out_dir\n","    self.epochs = epochs\n","    self.batch_size = batch_size\n","    self.learning_rate = learning_rate\n","    self.device = device\n","    self.dtype = dtype\n","    self.accumulation_steps = accumulation_steps\n","    self.grad_clip = grad_clip\n","    self.warmup_iters = warmup_iters\n","    self.log_interval = log_interval\n","    self.save_interval = save_interval\n","    self.local_rank = local_rank\n","    self.hidden_size = hidden_size\n","    self.num_hidden_layers = num_hidden_layers\n","    self.max_seq_len = max_seq_len\n","    self.data_path = data_path\n","    self.save_dir = None\n","    self.tokens_per_iter = self.batch_size * self.max_seq_len"],"metadata":{"id":"kTxviRsy9dgd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def output_logger():\n","    if step % args.log_interval == 0:\n","        spend_time = time.time() - start_time\n","        Logger(\n","            'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n","                epoch + 1,\n","                args.epochs,\n","                step,\n","                iter_per_epoch,\n","                loss.item() * args.accumulation_steps,\n","                optimizer.param_groups[-1]['lr'],\n","                spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))"],"metadata":{"id":"YbTru47_Sf5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_epoch_new(epoch, train_loader):\n","  # 1. 更新学习率\n","  # 2. 计算loss\n","  # 3. 更新参数\n","  # 初始化一个loss_function\n","  loss_fct = nn.CrossEntropyLoss(reduction='none')\n","  start_time = time.time()\n","\n","  # 取出X, Y, loss_mask\n","  for idx, (X, Y, loss_mask) in enumerate(train_loader):\n","    X = X.to(args.device)\n","    Y = Y.to(args.device)\n","    loss_mask = loss_mask.to(args.device)\n","\n","    # 更新学习率\n","    lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)\n","    for param_group in optim.param_groups:\n","      param_group['lr'] = lr\n","\n","    # 把计算过程用ctx套起来\n","    with ctx:\n","      # 计算掩码后loss\n","      result = model(X)\n","      loss = loss_fct(result.view(-1, result.logits.size(-1)), Y.view(-1)).view(Y.size())\n","      loss = (loss * loss_mask).sum() / loss_mask.sum()\n","      loss += result.aux_Loss\n","\n","      # 优化参数\n","      optim.zero_grad()\n","      scaler.scale(loss).backward()\n","      scaler.step(optim)\n","      scaler.update()\n","\n","      # 打印loss等\n","      output_logger()"],"metadata":{"id":"ElE0x9EcAsFj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 在首先复现了最简单的Logger、get_lr和init_model这三个函数之后，在main函数时，笔者卡住了。\n","倒不是看不懂代码，而是这套流程里调用了大量的包，一时不好复现（这要求你对这些包都有一定的了解）。\n","接着考虑，对代码进行删减，由于我们这里暂时不讨论分布式训练和对专家模型的复现，因此可以把这部分删除，然后开始学习main当中其他的包，应该如何使用。"],"metadata":{"id":"w9dBufvq6n6Q"}},{"cell_type":"code","source":["# 拆解后的代码如下：\n","if __name__ == \"__main__\":\n","    args = Arguments()\n","    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers)\n","    # args.save_dir = os.path.join(args.out_dir)\n","    # os.makedirs(args.save_dir, exist_ok=True)\n","    # os.makedirs(args.out_dir, exist_ok=True)\n","\n","    # token字典 长度设置\n","    device_type = args.device\n","\n","    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n","\n","    tokens_per_iter = args.tokens_per_iter\n","\n","    # 初始化模型和tokenizer\n","    model, tokenizer = init_model(lm_config)\n","    # 初始化Dataset和DataLoader\n","    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=args.batch_size,\n","        pin_memory=True,\n","        drop_last=False,\n","        shuffle=False,\n","        num_workers=args.num_workers\n","    )\n","    # 初始化scaler和optimizer\n","    scaler = torch.cuda.amp.GradScaler()\n","    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n","    iter_per_epoch = len(train_loader)\n","    for epoch in range(args.epochs):\n","        train_epoch_new(epoch, train_loader)\n","'''\n","在main中出现的非自定义函数还有：\n","torch.cuda.amp.autocast\n","PretrainDataset\n","DataLoader\n","torch.cuda.amp.GradScaler\n","optim\n","接下来我们逐个学习、熟悉一下它们。\n","'''"],"metadata":{"id":"bWw3zvRj9dlD","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1750928775062,"user_tz":-480,"elapsed":87,"user":{"displayName":"易文乾","userId":"13136328038158270412"}},"outputId":"cf18ff47-ef04-4362-a9ba-cacbe685a15e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n在main中出现的非自定义函数还有：\\ntorch.cuda.amp.autocast\\nPretrainDataset\\nDataLoader\\ntorch.cuda.amp.GradScaler\\noptim\\n接下来我们逐个学习、熟悉一下它们。\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]}]}